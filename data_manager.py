# data_manager.py
import datetime
import json
import time
import sqlite3
import threading
import uuid
import logging
import calendar
from collections import deque, OrderedDict
from contextlib import contextmanager
from zoneinfo import ZoneInfo, ZoneInfoNotFoundError
from dateutil import parser as dateutil_parser
from dateutil.relativedelta import relativedelta
from PyQt6.QtCore import QObject, pyqtSignal, QThread, QMutex, QMutexLocker, QTimer, QWaitCondition, QThreadPool, QRunnable

from providers.google_provider import GoogleCalendarProvider
from providers.local_provider import LocalCalendarProvider
from config import (DB_FILE, MAX_CACHE_SIZE, DEFAULT_SYNC_INTERVAL, 
                    GOOGLE_CALENDAR_PROVIDER_NAME, DEFAULT_EVENT_COLOR, DEFAULT_NOTIFICATIONS_ENABLED, 
                    DEFAULT_NOTIFICATION_MINUTES, DEFAULT_ALL_DAY_NOTIFICATION_ENABLED,
                    DEFAULT_ALL_DAY_NOTIFICATION_TIME)

logger = logging.getLogger(__name__)

def safe_json_dumps(obj):
    """Safely convert objects to JSON, handling datetime objects"""
    def json_serializer(obj):
        if isinstance(obj, (datetime.datetime, datetime.date)):
            return obj.isoformat()
        elif isinstance(obj, datetime.time):
            return obj.isoformat()
        elif hasattr(obj, 'zone') and hasattr(obj.zone, 'zone'):  # Handle timezone info
            return str(obj)
        raise TypeError(f"Object {obj} of type {type(obj)} is not JSON serializable")
    
    try:
        return json.dumps(obj, default=json_serializer, ensure_ascii=False)
    except Exception as e:
        logger.error(f"JSON ì§ë ¬í™” ì‹¤íŒ¨: {e}, object: {type(obj)}")
        return json.dumps({"error": "serialization_failed", "type": str(type(obj))})

def get_month_view_dates(year, month, start_day_of_week):
    """ì›”ê°„ ë·°ì— í‘œì‹œë  ëª¨ë“  ë‚ ì§œ(ì´ì „/í˜„ì¬/ë‹¤ìŒ ë‹¬ í¬í•¨)ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    first_day_of_month = datetime.date(year, month, 1)
    
    if start_day_of_week == 6: # ì¼ìš”ì¼ ì‹œì‘
        offset = (first_day_of_month.weekday() + 1) % 7
    else: # ì›”ìš”ì¼ ì‹œì‘
        offset = first_day_of_month.weekday()
        
    start_date = first_day_of_month - datetime.timedelta(days=offset)
    end_date = start_date + datetime.timedelta(days=41)
    return start_date, end_date

class DistanceBasedTaskQueue:
    """ê±°ë¦¬ë³„ ì‘ì—… ë¶„ë°° ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ê±°ë¦¬ë³„ í: 0(í˜„ì¬ì›”), 1(ì¸ì ‘ì›”) - 3ê°œì›” ìœˆë„ìš°ìš©
        self._queues = {i: deque() for i in range(2)}
        self._pending_tasks = set()
        # Python threading.Lockìœ¼ë¡œ QMutex ëŒ€ì²´ (ì•ˆì „í•œ ìŠ¤ë ˆë“œ ë™ê¸°í™”)
        self._lock = threading.Lock()
        # ê° ê±°ë¦¬ë³„ ì›Œì»¤ì˜ í™œì„± ì‘ì—… ì¶”ì 
        self._active_tasks = {i: None for i in range(2)}
        # ìµœê·¼ ì™„ë£Œëœ ì‘ì—… ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
        self._recently_completed = {}
        # ì™„ë£Œ ì¶”ì  ìœ ì§€ ì‹œê°„ (ì´ˆ)
        self._completion_cooldown = 30

    def _ensure_mutex_valid_DISABLED(self):
        """DISABLED - mutex runtime recreation was causing memory corruption"""
        # DISABLED - was causing memory corruption and crashes
            # # self._mutex = QMutex()  # DISABLED - dangerous pattern  # DISABLED - dangerous runtime recreation

    def add_task(self, distance, task_data):
        """ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—… ì¶”ê°€"""
        with self._lock:
            # ìµœê·¼ ì™„ë£Œëœ ì‘ì—…ì¸ì§€ í™•ì¸ (ì¿¨ë‹¤ìš´ ì²´í¬)
            import time
            current_time = time.time()
            if task_data in self._recently_completed:
                time_since_completion = current_time - self._recently_completed[task_data]
                if time_since_completion < self._completion_cooldown:
                    logger.debug(f"[ìºì‹œ DEBUG] ì‘ì—… ì¿¨ë‹¤ìš´ ì¤‘ - ìŠ¤í‚µ: {task_data} (ì™„ë£Œ í›„ {time_since_completion:.1f}ì´ˆ)")
                    return False
            
            # ê±°ë¦¬ ë²”ìœ„ ì œí•œ (0-1) - 3ê°œì›” ìœˆë„ìš°ìš©
            distance = min(max(distance, 0), 1)
            
            # ì´ë¯¸ pendingì´ì§€ë§Œ ë” ê°€ê¹Œìš´ ê±°ë¦¬ì¼ ë•Œ ê°±ì‹ 
            for d in sorted(self._queues.keys()):
                if task_data in self._queues[d] and distance < d:
                    self._queues[d].remove(task_data)
                    self._queues[distance].append(task_data)
                    logger.info(f"[ìºì‹œ DEBUG] ì‘ì—… ê±°ë¦¬ ê°±ì‹ : {task_data} ê±°ë¦¬{d}â†’{distance}")
                    return True
                    
            # ìƒˆ ì‘ì—… ì¶”ê°€
            if task_data not in self._pending_tasks:
                self._queues[distance].append(task_data)
                self._pending_tasks.add(task_data)
                logger.info(f"[ìºì‹œ DEBUG] ê±°ë¦¬{distance} ì‘ì—… ì¶”ê°€: {task_data}")
                return True
            return False

    def get_next_task_for_distance(self, distance):
        """íŠ¹ì • ê±°ë¦¬ì˜ ì›Œì»¤ê°€ ë‹¤ìŒ ì‘ì—… ê°€ì ¸ì˜¤ê¸°"""
        with self._lock:
            if distance in self._queues and self._queues[distance]:
                task_data = self._queues[distance].popleft()
                self._pending_tasks.discard(task_data)
                self._active_tasks[distance] = task_data
                return task_data
            return None

    def mark_task_completed(self, distance, task_data):
        """ì‘ì—… ì™„ë£Œ ì²˜ë¦¬"""
        with self._lock:
            if self._active_tasks.get(distance) == task_data:
                self._active_tasks[distance] = None
                logger.info(f"[ìºì‹œ DEBUG] ê±°ë¦¬{distance} ì‘ì—… ì™„ë£Œ: {task_data}")
            
            # ì™„ë£Œ ì‹œê°„ ê¸°ë¡ (ì¤‘ë³µ ë°©ì§€ìš©)
            import time
            self._recently_completed[task_data] = time.time()
            logger.debug(f"[ìºì‹œ DEBUG] ì‘ì—… ì™„ë£Œ ê¸°ë¡: {task_data} (30ì´ˆ ì¿¨ë‹¤ìš´ ì ìš©)")
            
            # ì˜¤ë˜ëœ ì™„ë£Œ ê¸°ë¡ ì •ë¦¬
            self._cleanup_old_completions()

    def interrupt_and_add_current_month(self, task_data):
        """í˜„ì¬ ì›”ì€ ëª¨ë“  ì‘ì—…ì„ ì¤‘ë‹¨í•˜ê³  ìµœìš°ì„  ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        with self._lock:
            # ì¿¨ë‹¤ìš´ ì²´í¬ - í˜„ì¬ì›”ë„ ì¤‘ë³µ ìš”ì²­ ë°©ì§€
            import time
            current_time = time.time()
            if task_data in self._recently_completed:
                time_since_completion = current_time - self._recently_completed[task_data]
                if time_since_completion < self._completion_cooldown:
                    logger.debug(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ì‘ì—… ì¿¨ë‹¤ìš´ ì¤‘ - ìŠ¤í‚µ: {task_data} (ì™„ë£Œ í›„ {time_since_completion:.1f}ì´ˆ)")
                    return False
            
            # 1. idle ì›Œì»¤ê°€ ìˆë‹¤ë©´ ìš°ì„  í™œìš© (ì¤‘ë‹¨ ë¶ˆí•„ìš”)
            if self._active_tasks[0] is None:  # ê±°ë¦¬0 ì›Œì»¤ê°€ idle
                self._queues[0].clear()
                self._queues[0].append(task_data)
                self._pending_tasks.add(task_data)
                logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” idle ì›Œì»¤ ì¦‰ì‹œ í• ë‹¹: {task_data}")
                return True
            
            # 2. ë‹¤ë¥¸ idle ì›Œì»¤ê°€ ìˆë‹¤ë©´ í™œìš©
            idle_workers = self.get_idle_workers()
            if idle_workers:
                # ê°€ì¥ ê°€ê¹Œìš´ idle ì›Œì»¤ ì„ íƒ (ê±°ë¦¬ 0ì— ê°€ì¥ ê°€ê¹Œìš´)
                optimal_worker = min(idle_workers)
                self._queues[optimal_worker].append(task_data)
                self._pending_tasks.add(task_data)
                logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ë‹¤ë¥¸ idle ì›Œì»¤ í™œìš©: ê±°ë¦¬{optimal_worker} â†’ {task_data}")
                return True
            
            # 3. ëª¨ë“  ì›Œì»¤ê°€ busyì¼ ë•Œë§Œ ì¤‘ë‹¨ ë¡œì§ ì‹¤í–‰
            # ê°€ì¥ ë¨¼ ê±°ë¦¬ ì›Œì»¤ë¥¼ ì¤‘ë‹¨
            farthest_worker = self.get_farthest_busy_worker()
            if farthest_worker is not None and farthest_worker > 0:  # ê±°ë¦¬0ì´ ì•„ë‹Œ ì›Œì»¤ë§Œ ì¤‘ë‹¨
                old_task = self._active_tasks[farthest_worker]
                if old_task:
                    self._pending_tasks.discard(old_task)
                    logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›”ì„ ìœ„í•´ ê±°ë¦¬{farthest_worker} ì›Œì»¤ ì¤‘ë‹¨: {old_task}")
                
                # ì¤‘ë‹¨ëœ ì›Œì»¤ì— í˜„ì¬ì›” í• ë‹¹
                self._queues[farthest_worker].clear()
                self._queues[farthest_worker].append(task_data)
                self._pending_tasks.add(task_data)
                self._active_tasks[farthest_worker] = None
                logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ì‘ì—…ì„ ê±°ë¦¬{farthest_worker} ì›Œì»¤ì— í• ë‹¹: {task_data}")
                return True
            
            # 4. ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ê±°ë¦¬0 ì›Œì»¤ ì¤‘ë‹¨ (ê¸°ì¡´ ë¡œì§)
            old_task = self._active_tasks[0]
            if old_task:
                self._pending_tasks.discard(old_task)
                logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ì‘ì—… ì¤‘ë‹¨: {old_task}")
            
            self._queues[0].clear()
            self._queues[0].append(task_data)
            self._pending_tasks.add(task_data)
            self._active_tasks[0] = None
            logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ìµœìš°ì„  ì‘ì—… ì„¤ì •: {task_data}")
            return True

    def clear_orphaned_pending(self):
        """íì—ëŠ” ì—†ì§€ë§Œ pending ìƒíƒœì¸ ê³ ì•„ ì‘ì—…ë“¤ ì •ë¦¬"""
        # self._ensure_mutex_valid()  # DISABLED - dangerous runtime mutex recreation
        # with QMutexLocker(self._mutex):  # DISABLED - was causing crashes
        if True:
            all_queued_tasks = set()
            for queue in self._queues.values():
                all_queued_tasks.update(queue)
            
            orphaned_tasks = self._pending_tasks - all_queued_tasks
            orphaned_count = len(orphaned_tasks)
            self._pending_tasks -= orphaned_tasks
            
            if orphaned_count > 0:
                logger.info(f"[ìºì‹œ DEBUG] ê³ ì•„ ì‘ì—… {orphaned_count}ê°œ ì •ë¦¬ë¨")
            return orphaned_count

    def _cleanup_old_completions(self):
        """ì˜¤ë˜ëœ ì™„ë£Œ ê¸°ë¡ ì •ë¦¬"""
        import time
        current_time = time.time()
        expired_keys = []
        
        for task_data, completion_time in self._recently_completed.items():
            if current_time - completion_time > self._completion_cooldown:
                expired_keys.append(task_data)
        
        for key in expired_keys:
            del self._recently_completed[key]
        
        if expired_keys:
            logger.debug(f"[ìºì‹œ DEBUG] ë§Œë£Œëœ ì™„ë£Œ ê¸°ë¡ {len(expired_keys)}ê°œ ì •ë¦¬ë¨")

    def get_queue_status(self):
        """ê° ê±°ë¦¬ë³„ í ìƒíƒœ ë°˜í™˜ (ë””ë²„ê·¸ìš©)"""
        # self._ensure_mutex_valid()  # DISABLED - dangerous runtime mutex recreation
        # with QMutexLocker(self._mutex):  # DISABLED - was causing crashes
        if True:
            status = {}
            for distance, queue in self._queues.items():
                status[distance] = {
                    'queued': len(queue),
                    'active': self._active_tasks[distance] is not None,
                    'active_task': self._active_tasks[distance]
                }
            return status

    def get_idle_workers(self):
        """í˜„ì¬ idle ìƒíƒœì¸ ì›Œì»¤ë“¤ì˜ ê±°ë¦¬ ëª©ë¡ ë°˜í™˜"""
        # ë½ì´ ì´ë¯¸ íšë“ëœ ìƒíƒœì—ì„œ í˜¸ì¶œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë½ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        idle_workers = []
        for distance in range(2):  # 0~1 ê±°ë¦¬
            if self._active_tasks[distance] is None:
                idle_workers.append(distance)
        logger.debug(f"[ìºì‹œ DEBUG] Idle ì›Œì»¤ë“¤: {idle_workers}")
        return idle_workers
    
    def find_optimal_idle_worker(self, target_distance):
        """ê°€ì¥ ì í•©í•œ idle ì›Œì»¤ ì°¾ê¸° (ê±°ë¦¬ ê¸°ì¤€)"""
        idle_workers = self.get_idle_workers()
        if not idle_workers:
            return None
        
        # íƒ€ê²Ÿ ê±°ë¦¬ì™€ ê°€ì¥ ê°€ê¹Œìš´ idle ì›Œì»¤ ì„ íƒ
        optimal_worker = min(idle_workers, key=lambda d: abs(d - target_distance))
        logger.info(f"[ìºì‹œ DEBUG] ìµœì  idle ì›Œì»¤ ì„ íƒ: ê±°ë¦¬{optimal_worker} (íƒ€ê²Ÿ ê±°ë¦¬{target_distance})")
        return optimal_worker
    
    def get_farthest_busy_worker(self):
        """ê°€ì¥ ë¨¼ ê±°ë¦¬ì—ì„œ ì‘ì—… ì¤‘ì¸ ì›Œì»¤ ì°¾ê¸°"""
        # ë½ì´ ì´ë¯¸ íšë“ëœ ìƒíƒœì—ì„œ í˜¸ì¶œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë½ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        busy_workers = []
        for distance in range(2):  # 0~1 ê±°ë¦¬
            if self._active_tasks[distance] is not None:
                busy_workers.append(distance)
        
        if not busy_workers:
            return None
        
        # ê°€ì¥ ë¨¼ ê±°ë¦¬ ì›Œì»¤ ë°˜í™˜
        farthest_worker = max(busy_workers)
        logger.debug(f"[ìºì‹œ DEBUG] ê°€ì¥ ë¨¼ busy ì›Œì»¤: ê±°ë¦¬{farthest_worker}")
        return farthest_worker
    
    def add_task_with_smart_assignment(self, distance, task_data):
        """ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ ì‘ì—… í• ë‹¹"""
        with self._lock:
            # 1. ì¿¨ë‹¤ìš´ ì²´í¬
            import time
            current_time = time.time()
            if task_data in self._recently_completed:
                time_since_completion = current_time - self._recently_completed[task_data]
                if time_since_completion < self._completion_cooldown:
                    logger.debug(f"[ìºì‹œ DEBUG] ì‘ì—… ì¿¨ë‹¤ìš´ ì¤‘ - ìŠ¤í‚µ: {task_data} (ì™„ë£Œ í›„ {time_since_completion:.1f}ì´ˆ)")
                    return False
            
            # 2. idle ì›Œì»¤ ìš°ì„  í™œìš© (ê°€ì¥ ì¤‘ìš”í•œ ê°œì„ ì )
            optimal_idle_worker = self.find_optimal_idle_worker(distance)
            if optimal_idle_worker is not None:
                # idle ì›Œì»¤ì— ì¦‰ì‹œ í• ë‹¹
                actual_distance = optimal_idle_worker
                self._queues[actual_distance].append(task_data)
                self._pending_tasks.add(task_data)
                logger.info(f"[ìºì‹œ DEBUG] idle ì›Œì»¤ ì¦‰ì‹œ í• ë‹¹: ê±°ë¦¬{actual_distance} â†’ {task_data}")
                return True
            
            # 3. ëª¨ë“  ì›Œì»¤ê°€ busyì¼ ë•Œ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            # ê±°ë¦¬ ë²”ìœ„ ì œí•œ (0-3)
            distance = min(max(distance, 0), 3)
            
            # ì´ë¯¸ pendingì´ì§€ë§Œ ë” ê°€ê¹Œìš´ ê±°ë¦¬ì¼ ë•Œ ê°±ì‹ 
            for d in sorted(self._queues.keys()):
                if task_data in self._queues[d] and distance < d:
                    self._queues[d].remove(task_data)
                    self._queues[distance].append(task_data)
                    logger.info(f"[ìºì‹œ DEBUG] ì‘ì—… ê±°ë¦¬ ê°±ì‹ : {task_data} ê±°ë¦¬{d}â†’{distance}")
                    return True
            
            # ìƒˆ ì‘ì—… ì¶”ê°€
            if task_data not in self._pending_tasks:
                self._queues[distance].append(task_data)
                self._pending_tasks.add(task_data)
                logger.info(f"[ìºì‹œ DEBUG] ìƒˆ ì‘ì—… ì¶”ê°€: ê±°ë¦¬{distance} â†’ {task_data}")
                return True
            
            logger.debug(f"[ìºì‹œ DEBUG] ì‘ì—… ì´ë¯¸ ì¡´ì¬: {task_data}")
            return False

    def __len__(self):
        # self._ensure_mutex_valid()  # DISABLED - dangerous runtime mutex recreation
        # with QMutexLocker(self._mutex):  # DISABLED - was causing crashes\n        # if True:
            return len(self._pending_tasks)

class DistanceWorker(QObject):
    """ê°œë³„ ì›Œì»¤ ìŠ¤ë ˆë“œ - íŠ¹ì • ê±°ë¦¬ì˜ ì‘ì—… ë‹´ë‹¹"""
    finished = pyqtSignal()
    
    def __init__(self, distance, task_queue, data_manager):
        super().__init__()
        self.distance = distance
        self.task_queue = task_queue
        self.data_manager = data_manager
        self._is_running = True
        
        # ê±°ë¦¬ë³„ ì›Œì»¤ ì´ë¦„ ì„¤ì •
        if distance == 0:
            self.name = "í˜„ì¬ì›”"
        else:
            self.name = f"ê±°ë¦¬{distance}"
            
        logger.info(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì´ˆê¸°í™”ë¨")

    def stop(self):
        """ì›Œì»¤ ì¤‘ì§€"""
        self._is_running = False
        logger.info(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì¤‘ì§€ ìš”ì²­ë¨")

    def run(self):
        """ì›Œì»¤ì˜ ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        logger.info(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì‹œì‘")
        
        while self._is_running:
            # 1. í• ë‹¹ëœ ê±°ë¦¬ì˜ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
            task_data = self.task_queue.get_next_task_for_distance(self.distance)
            
            if task_data:
                try:
                    task_type, (year, month) = task_data
                    logger.info(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì‘ì—… ì‹œì‘: {year}ë…„ {month}ì›”")
                    
                    # 2. ë™ê¸°í™” ìƒíƒœ ì•Œë¦¼ (UI ìŠ¤í”¼ë„ˆ í‘œì‹œ)
                    self.data_manager.set_sync_state(True, year, month)
                    
                    # 3. ì‹¤ì œ API í˜¸ì¶œ
                    events = self.data_manager._fetch_events_from_providers(year, month)
                    
                    if events is not None and self._is_running:
                        # 4. ìºì‹œì— ì €ì¥
                        self.data_manager._merge_events_preserving_temp(year, month, events)
                        self.data_manager._save_month_to_cache_db(year, month, events)
                        
                        # 5. UI ì—…ë°ì´íŠ¸ ì‹ í˜¸ ë°œì†¡
                        self.data_manager.data_updated.emit(year, month)
                        
                        logger.info(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì‘ì—… ì™„ë£Œ: {year}ë…„ {month}ì›” ({len(events)}ê°œ ì´ë²¤íŠ¸)")
                    
                    # 6. ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
                    self.data_manager.set_sync_state(False, year, month)
                    self.task_queue.mark_task_completed(self.distance, task_data)
                    
                except Exception as e:
                    logger.error(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì‘ì—… ì˜¤ë¥˜: {e}", exc_info=True)
                    # ì˜¤ë¥˜ ì‹œì—ë„ ë™ê¸°í™” ìƒíƒœ í•´ì œ
                    try:
                        task_type, (year, month) = task_data
                        self.data_manager.set_sync_state(False, year, month)
                    except:
                        pass
            else:
                # ì‘ì—…ì´ ì—†ìœ¼ë©´ ì ì‹œ ëŒ€ê¸°
                time.sleep(0.1)
        
        logger.info(f"[ìºì‹œ DEBUG] {self.name} ì›Œì»¤ ì¢…ë£Œë¨")
        self.finished.emit()


class DistanceBasedCachingManager(QObject):
    """7ê°œ ì›Œì»¤ ê¸°ë°˜ ê±°ë¦¬ë³„ ë³‘ë ¬ ìºì‹± ì‹œìŠ¤í…œ ì´ê´„ ê´€ë¦¬ì"""
    finished = pyqtSignal()

    def __init__(self, data_manager):
        super().__init__()
        self.data_manager = data_manager
        self._is_running = True
        # # self._mutex = QMutex()  # DISABLED - dangerous pattern  # DISABLED - dangerous runtime recreation
        
        # ê±°ë¦¬ë³„ ì‘ì—… í
        self._task_queue = DistanceBasedTaskQueue()
        self._last_viewed_month = None
        
        # 7ê°œ ì›Œì»¤ì™€ ìŠ¤ë ˆë“œ ê´€ë¦¬
        self._workers = {}
        self._worker_threads = {}
        
        # ì¼ì‹œì •ì§€ ê´€ë¦¬
        # self._activity_lock = QMutex()  # DISABLED - dangerous pattern
        self._pause_requested = False
        self._resume_condition = QWaitCondition()
        
        # 7ê°œ ì›Œì»¤ ì´ˆê¸°í™” (ê±°ë¦¬ 0~6)
        self._init_workers()
        
        logger.info("[ìºì‹œ DEBUG] DistanceBasedCachingManager ì´ˆê¸°í™” ì™„ë£Œ (7ê°œ ì›Œì»¤)")

    def _ensure_mutex_valid_DISABLED(self):
        """DISABLED - mutex runtime recreation was causing memory corruption"""
        # DISABLED - was causing memory corruption and crashes
            # # self._mutex = QMutex()  # DISABLED - dangerous pattern  # DISABLED - dangerous runtime recreation

    def _init_workers(self):
        """2ê°œ ê±°ë¦¬ë³„ ì›Œì»¤ ì´ˆê¸°í™” (3ê°œì›” ìœˆë„ìš°: í˜„ì¬ì›” + Â±1ê°œì›”)"""
        for distance in range(2):
            # ì›Œì»¤ ìƒì„±
            worker = DistanceWorker(distance, self._task_queue, self.data_manager)
            self._workers[distance] = worker
            
            # ìŠ¤ë ˆë“œ ìƒì„± ë° ì—°ê²°
            thread = QThread()
            self._worker_threads[distance] = thread
            
            worker.moveToThread(thread)
            thread.started.connect(worker.run)
            worker.finished.connect(thread.quit)
            worker.finished.connect(worker.deleteLater)
            thread.finished.connect(thread.deleteLater)
            
            # ìŠ¤ë ˆë“œ ì‹œì‘
            thread.start()
            
            logger.info(f"[ìºì‹œ DEBUG] ê±°ë¦¬{distance} ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")

    def request_caching_around(self, year, month, skip_current=False):
        """ê±°ë¦¬ë³„ ë³‘ë ¬ ìºì‹± ìš”ì²­"""
        # self._ensure_mutex_valid()  # DISABLED - dangerous runtime mutex recreation
        # with QMutexLocker(self._mutex):  # DISABLED - was causing crashes  # DISABLED FOR CRASH FIX
        if True:
            # ğŸš€ ì¤‘ë³µ ìš”ì²­ ë°©ì§€: ê°™ì€ ì›”ì— ëŒ€í•œ ìš”ì²­ì´ ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì´ë©´ ìŠ¤í‚µ
            current_request = (year, month)
            if hasattr(self, '_last_cache_request') and self._last_cache_request == current_request:
                # 100ms ì¿¨ë‹¤ìš´ ì²´í¬
                current_time = time.time()
                if hasattr(self, '_last_request_time'):
                    time_diff = current_time - self._last_request_time
                    if time_diff < 0.1:  # 100ms ì¿¨ë‹¤ìš´
                        logger.debug(f"[ìºì‹œ DEBUG] ì¤‘ë³µ ìš”ì²­ ë°©ì§€: {year}ë…„ {month}ì›” (ì¿¨ë‹¤ìš´ {time_diff:.3f}s)")
                        return
            
            self._last_cache_request = current_request
            self._last_request_time = time.time()
            self._last_viewed_month = (year, month)
            
            logger.info(f"[ìºì‹œ DEBUG] ë³‘ë ¬ ìºì‹± ìš”ì²­: {year}ë…„ {month}ì›” ì¤‘ì‹¬")
            
            # í˜„ì¬ ì›” ì²˜ë¦¬ (skip_currentê°€ Falseì¸ ê²½ìš°ì—ë§Œ)
            if not skip_current:
                current_month_task = ('month', (year, month))
                if self._task_queue.interrupt_and_add_current_month(current_month_task):
                    logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ìµœìš°ì„  ì²˜ë¦¬: {year}ë…„ {month}ì›”")
                else:
                    logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ì¿¨ë‹¤ìš´ìœ¼ë¡œ ìŠ¤í‚µ: {year}ë…„ {month}ì›”")

            # 1. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê³„ì‚° (3ê°œì›”: Â±1ê°œì›”)
            window_months = self._calculate_sliding_window(year, month, 1)
            
            # 2. ì´ë¯¸ ìºì‹œë˜ì§€ ì•Šì€ ì›”ë“¤ë§Œ í•„í„°ë§
            cached_months = set(self.data_manager.event_cache.keys())
            target_months = [m for m in window_months if m not in cached_months]
            
            # 3. í˜„ì¬ì›”ì„ ì´ë¯¸ ì²˜ë¦¬í–ˆë‹¤ë©´ target_monthsì—ì„œ ì œì™¸
            if not skip_current:
                current_month_key = (year, month)
                if current_month_key in target_months:
                    target_months.remove(current_month_key)
                    logger.debug(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ì¤‘ë³µ ì œê±°: {year}ë…„ {month}ì›”")
            
            logger.info(f"[ìºì‹œ DEBUG] ìœˆë„ìš° {len(window_months)}ê°œì›”, ë¯¸ìºì‹œ {len(target_months)}ê°œì›”")
            
            # 3. ê±°ë¦¬ë³„ ì‘ì—… ë¶„ì‚°
            distance_assignments = {}
            for target_month in target_months:
                # ê±°ë¦¬ ê³„ì‚° (ì ˆëŒ€ê°’) - 3ê°œì›” ìœˆë„ìš° ê¸°ì¤€
                distance = abs((target_month[0] - year) * 12 + (target_month[1] - month))
                worker_distance = min(distance, 1)  # ìµœëŒ€ ê±°ë¦¬ 1 (Â±1ê°œì›”)
                
                if worker_distance not in distance_assignments:
                    distance_assignments[worker_distance] = []
                distance_assignments[worker_distance].append(target_month)

            # 4. ê° ê±°ë¦¬ë³„ ì›Œì»¤ì— ì‘ì—… í• ë‹¹
            total_assigned = 0
            for distance, months in distance_assignments.items():
                for target_month in months:
                    task_data = ('month', target_month)
                    added = self._task_queue.add_task_with_smart_assignment(distance, task_data)
                    
                    if added:
                        total_assigned += 1
                        logger.info(f"[ìºì‹œ DEBUG] ê±°ë¦¬{distance} ì›Œì»¤ í• ë‹¹: {target_month}")

            logger.info(f"[ìºì‹œ DEBUG] ì´ {total_assigned}ê°œ ì‘ì—…ì´ 2ê°œ ì›Œì»¤ì— ë¶„ì‚° í• ë‹¹ë¨")
            
            # 5. ê³ ì•„ ì‘ì—… ì •ë¦¬
            orphaned_count = self._task_queue.clear_orphaned_pending()
            if orphaned_count > 0:
                logger.info(f"[ìºì‹œ DEBUG] {orphaned_count}ê°œ ê³ ì•„ ì‘ì—… ì •ë¦¬ë¨")

    def request_current_month_sync(self):
        """í˜„ì¬ ì›” ë™ê¸°í™” ìš”ì²­"""
        # self._ensure_mutex_valid()  # DISABLED - dangerous runtime mutex recreation
        # with QMutexLocker(self._mutex):  # DISABLED - was causing crashes
        if True:
            if self._last_viewed_month:
                task_data = ("month", self._last_viewed_month)
                # í˜„ì¬ ì›”ì€ í•­ìƒ ê±°ë¦¬ 0ìœ¼ë¡œ ì²˜ë¦¬
                self._task_queue.add_task(0, task_data)
                logger.info(f"[ìºì‹œ DEBUG] í˜„ì¬ì›” ë™ê¸°í™” ìš”ì²­: {self._last_viewed_month}")

    def stop(self):
        """ëª¨ë“  ì›Œì»¤ ì¤‘ì§€"""
        logger.info("[ìºì‹œ DEBUG] ëª¨ë“  ì›Œì»¤ ì¤‘ì§€ ìš”ì²­")
        self._is_running = False
        
        # ëª¨ë“  ì›Œì»¤ ì¤‘ì§€
        for worker in self._workers.values():
            worker.stop()
        
        self.resume_sync()  # ì¼ì‹œì •ì§€ ìƒíƒœ í•´ì œ
        
        # ëª¨ë“  ì›Œì»¤ ì¢…ë£Œ ëŒ€ê¸°
        for thread in self._worker_threads.values():
            if thread.isRunning():
                thread.quit()
                thread.wait(3000)  # 3ì´ˆ ëŒ€ê¸°
        
        logger.info("[ìºì‹œ DEBUG] ëª¨ë“  ì›Œì»¤ ì¤‘ì§€ ì™„ë£Œ")
        self.finished.emit()

    def _calculate_sliding_window(self, center_year, center_month, radius):
        """ì¤‘ì‹¬ ì›” ê¸°ì¤€ìœ¼ë¡œ Â±radius ê°œì›”ì˜ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê³„ì‚° (ì¤‘ì‹¬ì›” í¬í•¨)"""
        months = []
        center_date = datetime.date(center_year, center_month, 1)
        
        for i in range(-radius, radius + 1):
            # ì¤‘ì‹¬ì›”ë„ í¬í•¨í•˜ì—¬ 7ê°œì›” ìœˆë„ìš° êµ¬ì„± (Â±3ê°œì›”)
            target_date = center_date + relativedelta(months=i)
            months.append((target_date.year, target_date.month))
        
        return months

    def pause_sync(self):
        """ëª¨ë“  ì›Œì»¤ ì¼ì‹œì •ì§€"""
        # _activity_lock disabled for safety - using simple flag instead
        self._pause_requested = True
        logger.info("[ìºì‹œ DEBUG] ëª¨ë“  ì›Œì»¤ ì¼ì‹œì •ì§€ ìš”ì²­")

    def resume_sync(self):
        """ëª¨ë“  ì›Œì»¤ ì¬ê°œ"""
        if self._pause_requested:
            self._pause_requested = False
            # _activity_lock and _resume_condition disabled for safety - using simple flag instead
            logger.info("[ìºì‹œ DEBUG] ëª¨ë“  ì›Œì»¤ ì¬ê°œë¨")

    def get_queue_status(self):
        """í ìƒíƒœ ì¡°íšŒ (ë””ë²„ê·¸ìš©)"""
        return self._task_queue.get_queue_status()

    def get_worker_status(self):
        """ì›Œì»¤ ìƒíƒœ ì¡°íšŒ (ë””ë²„ê·¸ìš©)"""
        status = {}
        for distance, thread in self._worker_threads.items():
            status[distance] = {
                'thread_running': thread.isRunning(),
                'worker_name': self._workers[distance].name
            }
        return status

class CalendarListFetcher(QObject):
    calendars_fetched = pyqtSignal(list)
    finished = pyqtSignal()

    def __init__(self, providers):
        super().__init__()
        self.providers = providers
        self._is_running = True

    def run(self):
        logger.info("ìº˜ë¦°ë” ëª©ë¡ ë¹„ë™ê¸° ë¡œë” ìŠ¤ë ˆë“œ ì‹œì‘...")
        all_calendars = []
        for provider in self.providers:
            if not self._is_running: break
            if hasattr(provider, 'get_calendars'):
                try:
                    all_calendars.extend(provider.get_calendars())
                except Exception as e:
                    logger.error(f"'{type(provider).__name__}'ì—ì„œ ìº˜ë¦°ë” ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)
        
        if self._is_running:
            self.calendars_fetched.emit(all_calendars)
        
        self.finished.emit()
        logger.info("ìº˜ë¦°ë” ëª©ë¡ ë¹„ë™ê¸° ë¡œë” ìŠ¤ë ˆë“œ ì¢…ë£Œ.")

    def stop(self):
        self._is_running = False

class DataManager(QObject):
    data_updated = pyqtSignal(int, int)
    calendar_list_changed = pyqtSignal()
    event_completion_changed = pyqtSignal()
    error_occurred = pyqtSignal(str)
    notification_triggered = pyqtSignal(str, str)
    # [ìˆ˜ì •] is_syncing ìƒíƒœ, ë…„, ì›” ì •ë³´ë¥¼ í•¨ê»˜ ì „ë‹¬
    sync_state_changed = pyqtSignal(bool, int, int) 

    def __init__(self, settings, auth_manager, start_timer=True, load_cache=True):
        super().__init__()
        self.settings = settings
        self.auth_manager = auth_manager
        self.auth_manager.auth_state_changed.connect(self.on_auth_state_changed)

        # [ìˆ˜ì •] is_syncingì„ ì›”ë³„ë¡œ ê´€ë¦¬í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ê²½
        self.syncing_months = {}
        self.event_cache = {}
        self.completed_event_ids = set()
        self.notified_event_ids = set()
        
        # [ì¶”ê°€] ë¡œì»¬-í¼ìŠ¤íŠ¸ ì‚­ì œ ì¶”ì : ë°±ê·¸ë¼ìš´ë“œ ì‚­ì œ ì¤‘ì¸ ì´ë²¤íŠ¸ ID ì¶”ì 
        self.pending_deletion_ids = set()  # í˜„ì¬ ì‚­ì œ ì¤‘ì¸ ì´ë²¤íŠ¸ IDë“¤
        self.batch_deletion_mode = False  # ë°°ì¹˜ ì‚­ì œ ëª¨ë“œ í”Œë˜ê·¸
        
        self.calendar_list_cache = None
        self._default_color_map_cache = None
        
        # [ì‚­ì œ] ImmediateSyncWorker ê´€ë ¨ ë©¤ë²„ ë³€ìˆ˜ ì‚­ì œ
        
        # ìºì‹œ ìœˆë„ìš° ë³€í™” ì¶”ì 
        self.last_cache_window = set()  # ë§ˆì§€ë§‰ ìºì‹œ ìœˆë„ìš°
        self.current_view_month = None  # ì‚¬ìš©ìê°€ í˜„ì¬ ë³´ê³  ìˆëŠ” ì›”
        
        self.calendar_fetch_thread = None
        self.calendar_fetcher = None

        if load_cache:
            # ìƒˆë¡œìš´ ë¶„ë¦¬ëœ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            self._init_separated_db_system()
            self._load_cache_from_db()
            self._init_completed_events_db()
            self._load_completed_event_ids()
        
        self.last_requested_month = None
        self.providers = []
        # DistanceBasedCachingManagerëŠ” ìì²´ì ìœ¼ë¡œ 7ê°œ ì›Œì»¤ ìŠ¤ë ˆë“œë¥¼ ê´€ë¦¬
        self.caching_manager = DistanceBasedCachingManager(self)
        
        if start_timer:
            self.sync_timer = QTimer(self)
            self.update_sync_timer()

            self.notification_timer = QTimer(self)
            self.notification_timer.timeout.connect(self._check_for_notifications)
            self.notification_timer.start(60 * 1000)
            
            # ìŠ¤ë§ˆíŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ íƒ€ì´ë¨¸ (ì¡°ê±´ë¶€ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”)
            self.smart_update_timer = QTimer(self)
            self.smart_update_timer.timeout.connect(self._smart_realtime_update)
            self.smart_update_timer.start(3 * 1000)  # 3ì´ˆë§ˆë‹¤ ì²´í¬
            self.last_update_time = time.time()
            logger.info("ìŠ¤ë§ˆíŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ íƒ€ì´ë¨¸ ì‹œì‘ë¨: 3ì´ˆ ê°„ê²©")
        
        self.setup_providers()

    # [ì¶”ê°€] CachingManagerê°€ í˜¸ì¶œí•  ë™ê¸°í™” ìƒíƒœ ì„¤ì • ë©”ì„œë“œ
    def set_sync_state(self, is_syncing, year, month):
        self.syncing_months[(year, month)] = is_syncing
        self.sync_state_changed.emit(is_syncing, year, month)

    def is_month_syncing(self, year, month):
        return self.syncing_months.get((year, month), False)

    def report_error(self, message):
        self.error_occurred.emit(message)

    def get_color_for_calendar(self, cal_id):
        custom_colors = self.settings.get("calendar_colors", {})
        if cal_id in custom_colors:
            return custom_colors[cal_id]

        if not hasattr(self, '_default_color_map_cache') or self._default_color_map_cache is None:
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            self._default_color_map_cache = {cal['id']: cal.get('backgroundColor', DEFAULT_EVENT_COLOR) for cal in all_calendars}
        
        return self._default_color_map_cache.get(cal_id, DEFAULT_EVENT_COLOR)

    def _init_completed_events_db(self):
        try:
            with sqlite3.connect(DB_FILE) as conn:
                cursor = conn.cursor()
                cursor.execute("CREATE TABLE IF NOT EXISTS completed_events (event_id TEXT PRIMARY KEY)")
                conn.commit()
        except sqlite3.Error as e:
            msg = "ì™„ë£Œ ì´ë²¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            logger.error(msg, exc_info=True)
            self.report_error(f"{msg}\n{e}")

    def _load_completed_event_ids(self):
        try:
            with sqlite3.connect(DB_FILE) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT event_id FROM completed_events")
                self.completed_event_ids = {row[0] for row in cursor.fetchall()}
            logger.info(f"DBì—ì„œ {len(self.completed_event_ids)}ê°œì˜ ì™„ë£Œëœ ì´ë²¤íŠ¸ ìƒíƒœë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except sqlite3.Error as e:
            msg = "ì™„ë£Œ ì´ë²¤íŠ¸ ìƒíƒœë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            logger.error(msg, exc_info=True)
            self.report_error(f"{msg}\n{e}")

    def is_event_completed(self, event_id):
        return event_id in self.completed_event_ids

    def mark_event_as_completed(self, event_id):
        if event_id in self.completed_event_ids: return
        try:
            with sqlite3.connect(DB_FILE) as conn:
                cursor = conn.cursor()
                cursor.execute("INSERT OR IGNORE INTO completed_events (event_id) VALUES (?)", (event_id,))
                conn.commit()
            self.completed_event_ids.add(event_id)
            self.event_completion_changed.emit()
            logger.info(f"ì´ë²¤íŠ¸ {event_id}ë¥¼ ì™„ë£Œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        except sqlite3.Error as e:
            msg = f"ì´ë²¤íŠ¸({event_id})ë¥¼ ì™„ë£Œ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            logger.error(msg, exc_info=True)
            self.report_error(f"{msg}\n{e}")

    def unmark_event_as_completed(self, event_id):
        if event_id not in self.completed_event_ids: return
        try:
            with sqlite3.connect(DB_FILE) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM completed_events WHERE event_id = ?", (event_id,))
                conn.commit()
            self.completed_event_ids.discard(event_id)
            self.event_completion_changed.emit()
            logger.info(f"ì´ë²¤íŠ¸ {event_id}ë¥¼ ì§„í–‰ ì¤‘ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")
        except sqlite3.Error as e:
            msg = f"ì´ë²¤íŠ¸({event_id})ë¥¼ ì§„í–‰ ì¤‘ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            logger.error(msg, exc_info=True)
            self.report_error(f"{msg}\n{e}")

    def _init_separated_db_system(self):
        """ìƒˆë¡œìš´ ë¶„ë¦¬ëœ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            from db_manager import get_db_manager
            db_manager = get_db_manager()
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
            # db_managerì˜ __init__ì—ì„œ ìë™ìœ¼ë¡œ _init_databases()ì™€ migrate_existing_data()ê°€ í˜¸ì¶œë¨
            logger.info("ë¶„ë¦¬ëœ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì´ˆê¸°í™” í›„ ì¦‰ì‹œ ìºì‹œ ì •ë¦¬ ìˆ˜í–‰ (ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€)
            deleted_count = db_manager.cleanup_old_cache()
            if deleted_count > 0:
                logger.info(f"ì´ˆê¸°í™” ì‹œ ìºì‹œ ì •ë¦¬: {deleted_count}ê°œ ì—”íŠ¸ë¦¬ ì‚­ì œë¨")
                
        except Exception as e:
            msg = "ë¶„ë¦¬ëœ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            logger.error(msg, exc_info=True)
            self.report_error(f"{msg}\n{e}")

    def setup_providers(self):
        self.providers = []
        if self.auth_manager.is_logged_in():
            try:
                google_provider = GoogleCalendarProvider(self.settings, self.auth_manager)
                self.providers.append(google_provider)
            except Exception as e:
                logger.error("Google Provider ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)
        try:
            local_provider = LocalCalendarProvider(self.settings)
            self.providers.append(local_provider)
        except Exception as e:
            logger.error("Local Provider ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)

    def on_auth_state_changed(self):
        logger.info("ì¸ì¦ ìƒíƒœ ë³€ê²½ ê°ì§€. Providerë¥¼ ì¬ì„¤ì •í•˜ê³  ë°ì´í„°ë¥¼ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤.")
        
        is_logging_out = not self.auth_manager.is_logged_in()
        
        self.setup_providers()
        
        # ìƒ‰ìƒ ë§µ ìºì‹œë§Œ ì´ˆê¸°í™” (ìº˜ë¦°ë” ëª©ë¡ì€ ë™ê¸°ì ìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±ë¨)
        self._default_color_map_cache = None
        
        if is_logging_out:
            logger.info("ë¡œê·¸ì•„ì›ƒ ê°ì§€. Google ìº˜ë¦°ë” ê´€ë ¨ ìºì‹œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
            for month_key, events in list(self.event_cache.items()):
                google_events = [e for e in events if e.get('provider') == GOOGLE_CALENDAR_PROVIDER_NAME]
                if google_events:
                    remaining_events = [e for e in events if e.get('provider') != GOOGLE_CALENDAR_PROVIDER_NAME]
                    self.event_cache[month_key] = remaining_events
                    self._save_month_to_cache_db(month_key[0], month_key[1], remaining_events)
        
        # ë¡œê·¸ì¸ í›„ ìº˜ë¦°ë” ëª©ë¡ê³¼ í˜„ì¬ ì›” ë°ì´í„°ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨
        if not is_logging_out and self.auth_manager.is_logged_in():
            logger.info("ë¡œê·¸ì¸ ì™„ë£Œ í›„ ìº˜ë¦°ë” ëª©ë¡ê³¼ ì´ë²¤íŠ¸ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì‹œì‘...")
            
            # ìº˜ë¦°ë” ëª©ë¡ì„ ë¨¼ì € ìƒˆë¡œê³ ì¹¨ (ë™ê¸°ì ìœ¼ë¡œ)
            self._fetch_calendars_async()
            
            # í˜„ì¬ ì›” ìºì‹œ ì‚­ì œ í›„ ê°•ì œ ë™ê¸°í™”
            if self.last_requested_month:
                year, month = self.last_requested_month
                self.event_cache.pop((year, month), None)
                logger.info(f"ë¡œê·¸ì¸ í›„ í˜„ì¬ ì›” ìºì‹œ ì‚­ì œ ë° ê°•ì œ ë™ê¸°í™”: {year}ë…„ {month}ì›”")
                
                # QTimerë¥¼ ì‚¬ìš©í•´ ìº˜ë¦°ë” ëª©ë¡ ë¡œë”© ì™„ë£Œ í›„ ì´ë²¤íŠ¸ ë™ê¸°í™”
                from PyQt6.QtCore import QTimer
                def delayed_sync():
                    logger.info(f"ë¡œê·¸ì¸ í›„ ì§€ì—°ëœ ì´ë²¤íŠ¸ ë™ê¸°í™” ì‹œì‘: {year}ë…„ {month}ì›”")
                    # ë¡œê·¸ì¸ í›„ í™•ì‹¤í•œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸ ì‹ í˜¸ ë°œì†¡
                    self.data_updated.emit(year, month)
                    # ê·¸ë¦¬ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìµœì‹  ë°ì´í„° ë™ê¸°í™”
                    self.force_sync_month(year, month)
                
                QTimer.singleShot(500, delayed_sync)  # 500ms í›„ ì‹¤í–‰ (ë” ì—¬ìœ ìˆê²Œ)
            else:
                today = datetime.date.today()
                self.event_cache.pop((today.year, today.month), None)
                logger.info(f"ë¡œê·¸ì¸ í›„ ì˜¤ëŠ˜ ë‚ ì§œ ìºì‹œ ì‚­ì œ ë° ê°•ì œ ë™ê¸°í™”: {today.year}ë…„ {today.month}ì›”")
                
                # QTimerë¥¼ ì‚¬ìš©í•´ ìº˜ë¦°ë” ëª©ë¡ ë¡œë”© ì™„ë£Œ í›„ ì´ë²¤íŠ¸ ë™ê¸°í™”
                from PyQt6.QtCore import QTimer
                def delayed_sync():
                    logger.info(f"ë¡œê·¸ì¸ í›„ ì§€ì—°ëœ ì´ë²¤íŠ¸ ë™ê¸°í™” ì‹œì‘: {today.year}ë…„ {today.month}ì›”")
                    # ë¡œê·¸ì¸ í›„ í™•ì‹¤í•œ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸ ì‹ í˜¸ ë°œì†¡
                    self.data_updated.emit(today.year, today.month)
                    # ê·¸ë¦¬ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìµœì‹  ë°ì´í„° ë™ê¸°í™”
                    self.force_sync_month(today.year, today.month)
                
                QTimer.singleShot(500, delayed_sync)  # 500ms í›„ ì‹¤í–‰ (ë” ì—¬ìœ ìˆê²Œ)
        elif self.last_requested_month:
            year, month = self.last_requested_month
            self.data_updated.emit(year, month)
        else:
            today = datetime.date.today()
            self.data_updated.emit(today.year, today.month)

    def update_sync_timer(self):
        interval_minutes = self.settings.get("sync_interval_minutes", DEFAULT_SYNC_INTERVAL)
        if interval_minutes > 0:
            self.sync_timer.start(interval_minutes * 60 * 1000)
            logger.info(f"ìë™ ë™ê¸°í™” íƒ€ì´ë¨¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ê¸°: {interval_minutes}ë¶„")
        else:
            self.sync_timer.stop()
            logger.info("ìë™ ë™ê¸°í™”ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def request_current_month_sync(self):
        self.caching_manager.request_current_month_sync()

    def _smart_realtime_update(self):
        """ìŠ¤ë§ˆíŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ - ì¡°ê±´ë¶€ë¡œë§Œ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”"""
        current_time = time.time()
        
        # ì¡°ê±´ 1: ë¡œê·¸ì¸ë˜ì–´ ìˆì„ ë•Œë§Œ
        if not self.auth_manager.is_logged_in():
            return
            
        # ì¡°ê±´ 2: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ë¡œë¶€í„° ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚¬ì„ ë•Œë§Œ
        if current_time - self.last_update_time < 5:  # 5ì´ˆ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ (ì‹¤ì‹œê°„ì„± ìš°ì„ )
            return
            
        # ì¡°ê±´ 3: í˜„ì¬ ë™ê¸°í™” ì¤‘ì¸ ì›”ì´ ì—†ì„ ë•Œë§Œ
        if self.last_requested_month:
            year, month = self.last_requested_month
            if self.is_month_syncing(year, month):
                logger.debug(f"ìŠ¤ë§ˆíŠ¸ ì—…ë°ì´íŠ¸ ìŠ¤í‚µ: {year}ë…„ {month}ì›” ë™ê¸°í™” ì¤‘")
                return
        else:
            today = datetime.date.today()
            year, month = today.year, today.month
            if self.is_month_syncing(year, month):
                logger.debug(f"ìŠ¤ë§ˆíŠ¸ ì—…ë°ì´íŠ¸ ìŠ¤í‚µ: {year}ë…„ {month}ì›” ë™ê¸°í™” ì¤‘")
                return
                
        # ì‹¤ì œ ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™” ì‹¤í–‰ (UI ì—…ë°ì´íŠ¸ëŠ” ì™„ë£Œ ì‹œ ìë™ ë°œìƒ)
        logger.debug(f"ìŠ¤ë§ˆíŠ¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹¤í–‰: {year}ë…„ {month}ì›”")
        self.force_sync_month(year, month)
        self.last_update_time = current_time

    def notify_date_changed(self, new_date):
        self.last_requested_month = (new_date.year, new_date.month)
        self.caching_manager.request_caching_around(new_date.year, new_date.month)

    def stop_caching_thread(self):
        """ğŸ›‘ ì™„ì „í•œ ì¢…ë£Œ: ëª¨ë“  ìŠ¤ë ˆë“œì™€ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì •ë¦¬"""
        logger.info("ğŸ›‘ [SHUTDOWN] DCWidget ì¢…ë£Œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        try:
            # 1. íƒ€ì´ë¨¸ë“¤ ì¤‘ì§€
            if hasattr(self, 'notification_timer'):
                self.notification_timer.stop()
                logger.info("ğŸ“± ì•Œë¦¼ íƒ€ì´ë¨¸ ì¤‘ì§€ë¨")
            
            if hasattr(self, 'smart_update_timer'):
                self.smart_update_timer.stop()
                logger.info("âš¡ ìŠ¤ë§ˆíŠ¸ ì—…ë°ì´íŠ¸ íƒ€ì´ë¨¸ ì¤‘ì§€ë¨")
            
            # 2. DistanceBasedCachingManager ì¤‘ì§€ (4ê°œ ì›Œì»¤ ìŠ¤ë ˆë“œ)
            if hasattr(self, 'caching_manager') and self.caching_manager:
                logger.info("ğŸ”„ ìºì‹± ë§¤ë‹ˆì € ì¤‘ì§€ ì¤‘...")
                self.caching_manager.stop()
                logger.info("âœ… ìºì‹± ë§¤ë‹ˆì € ì¤‘ì§€ ì™„ë£Œ")
            
            # 3. ìº˜ë¦°ë” í˜ì¹˜ ìŠ¤ë ˆë“œ ì¤‘ì§€
            if hasattr(self, 'calendar_fetch_thread') and self.calendar_fetch_thread is not None:
                if self.calendar_fetch_thread.isRunning():
                    logger.info("ğŸ“… ìº˜ë¦°ë” í˜ì¹˜ ìŠ¤ë ˆë“œ ì¤‘ì§€ ì¤‘...")
                    if hasattr(self, 'calendar_fetcher'):
                        self.calendar_fetcher.stop()
                    self.calendar_fetch_thread.quit()
                    if not self.calendar_fetch_thread.wait(3000):  # 3ì´ˆ ëŒ€ê¸°
                        logger.warning("âš ï¸ ìº˜ë¦°ë” í˜ì¹˜ ìŠ¤ë ˆë“œ ê°•ì œ ì¢…ë£Œ")
                        self.calendar_fetch_thread.terminate()
                    logger.info("âœ… ìº˜ë¦°ë” í˜ì¹˜ ìŠ¤ë ˆë“œ ì¤‘ì§€ ì™„ë£Œ")
            
            # 4. QThreadPool ì‘ì—…ë“¤ ëŒ€ê¸° ë° ì¢…ë£Œ
            logger.info("ğŸ§µ QThreadPool ì‘ì—… ëŒ€ê¸° ì¤‘...")
            thread_pool = QThreadPool.globalInstance()
            thread_pool.waitForDone(5000)  # 5ì´ˆ ëŒ€ê¸°
            if thread_pool.activeThreadCount() > 0:
                logger.warning(f"âš ï¸ {thread_pool.activeThreadCount()}ê°œ ìŠ¤ë ˆë“œê°€ ì—¬ì „íˆ í™œì„± ìƒíƒœ")
            else:
                logger.info("âœ… ëª¨ë“  QThreadPool ì‘ì—… ì™„ë£Œ")
            
            # 5. Providerë“¤ ì •ë¦¬
            if hasattr(self, 'providers'):
                for provider in self.providers:
                    if hasattr(provider, 'cleanup'):
                        try:
                            provider.cleanup()
                        except Exception as e:
                            logger.warning(f"âš ï¸ Provider ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.info("âœ… Providerë“¤ ì •ë¦¬ ì™„ë£Œ")
            
            logger.info("ğŸ‰ [SHUTDOWN] DCWidget ì¢…ë£Œ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ [SHUTDOWN] ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

    @contextmanager
    def user_action_priority(self):
        self.caching_manager.pause_sync()
        try:
            yield
        finally:
            self.caching_manager.resume_sync()

    def _load_cache_from_db(self):
        try:
            # ìƒˆë¡œìš´ ë¶„ë¦¬ëœ ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
            from db_manager import get_db_manager
            db_manager = get_db_manager()
            
            with db_manager.get_cache_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT year, month, events_json FROM event_cache")
                for year, month, events_json in cursor.fetchall():
                    self.event_cache[(year, month)] = json.loads(events_json)
            logger.info(f"ìºì‹œ DBì—ì„œ {len(self.event_cache)}ê°œì˜ ì›”ê°„ ìºì‹œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            # ì‹œì‘ ì‹œ ë©”ëª¨ë¦¬-DB ìºì‹œ ë™ê¸°í™” í™•ì¸
            self._sync_memory_cache_with_db()
        except sqlite3.Error as e:
            msg = "ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìºì‹œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            logger.error(msg, exc_info=True)
            self.report_error(f"{msg}\n{e}")

    def _save_month_to_cache_db(self, year, month, events):
        try:
            # ìƒˆë¡œìš´ ë¶„ë¦¬ëœ ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
            from db_manager import get_db_manager
            db_manager = get_db_manager()
            
            with db_manager.get_cache_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("INSERT OR REPLACE INTO event_cache (year, month, events_json, cached_at) VALUES (?, ?, ?, CURRENT_TIMESTAMP)", (year, month, safe_json_dumps(events)))
                conn.commit()
            
            # ìºì‹œ ì •ë¦¬ëŠ” ì‚¬ìš©ìì˜ ì›” ì´ë™ ì‹œì—ë§Œ ìˆ˜í–‰ (UIì—ì„œ íŠ¸ë¦¬ê±°)
            
        except sqlite3.Error as e:
            logger.error("ìºì‹œ DBì— ì›”ê°„ ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)

    def _remove_month_from_cache_db(self, year, month):
        try:
            # ìƒˆë¡œìš´ ë¶„ë¦¬ëœ ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
            from db_manager import get_db_manager
            db_manager = get_db_manager()
            
            with db_manager.get_cache_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM event_cache WHERE year = ? AND month = ?", (year, month))
                conn.commit()
        except sqlite3.Error as e:
            logger.error("ìºì‹œ DBì—ì„œ ì›”ê°„ ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)
    
    def _schedule_cache_cleanup(self, center_year=None, center_month=None):
        """ìºì‹œ ì •ë¦¬ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜í–‰ (ìœˆë„ìš° ë³€í™”ì‹œì—ë§Œ)"""
        try:
            # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
            if center_year is None or center_month is None:
                import datetime
                today = datetime.date.today()
                center_year, center_month = today.year, today.month
            
            # ìºì‹œ ìœˆë„ìš° ë³€í™” í™•ì¸
            window_changed, current_window = self._check_cache_window_changed(center_year, center_month)
            
            # ë³€í™”ê°€ ì—†ìœ¼ë©´ ì •ë¦¬í•˜ì§€ ì•ŠìŒ
            if not window_changed:
                logger.debug(f"[ìºì‹œ ì •ë¦¬] ìœˆë„ìš° ë³€í™” ì—†ìŒ - ì •ë¦¬ ìŠ¤í‚µ")
                return
            
            import threading
            
            def cleanup_task():
                try:
                    from db_manager import get_db_manager
                    db_manager = get_db_manager()
                    
                    # ìœˆë„ìš° ë³€í™”ê°€ ìˆì„ ë•Œë§Œ ì •ë¦¬ ì‹¤í–‰
                    deleted_count = db_manager.cleanup_old_cache(center_year, center_month)
                    if deleted_count > 0:
                        logger.info(f"[ìºì‹œ ì •ë¦¬] ìœˆë„ìš° ë³€í™”ë¡œ ì¸í•œ ì •ë¦¬: {deleted_count}ê°œ ì—”íŠ¸ë¦¬ ì‚­ì œë¨")
                        # DB ìºì‹œ ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìºì‹œì™€ ë™ê¸°í™”
                        self._sync_memory_cache_with_db()
                    else:
                        logger.info(f"[ìºì‹œ ì •ë¦¬] ìœˆë„ìš° ë³€í™” ìˆìœ¼ë‚˜ ì‚­ì œí•  í•­ëª© ì—†ìŒ")
                        
                except Exception as e:
                    logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (UI ë¸”ë¡œí‚¹ ë°©ì§€)
            cleanup_thread = threading.Thread(target=cleanup_task, daemon=True, name="CacheCleanup")
            cleanup_thread.start()
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨: {e}")

    def _sync_memory_cache_with_db(self):
        """ë©”ëª¨ë¦¬ ìºì‹œë¥¼ DB ìºì‹œì™€ ë™ê¸°í™”"""
        try:
            from db_manager import get_db_manager
            db_manager = get_db_manager()
            
            # DBì—ì„œ í˜„ì¬ ìºì‹œëœ ì›” ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            with db_manager.get_cache_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT year, month FROM event_cache")
                db_cached_months = set((row[0], row[1]) for row in cursor.fetchall())
            
            # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ DBì— ì—†ëŠ” í•­ëª© ì œê±°
            memory_months = set(self.event_cache.keys())
            to_remove = memory_months - db_cached_months
            
            removed_count = 0
            for year, month in to_remove:
                if (year, month) in self.event_cache:
                    del self.event_cache[(year, month)]
                    removed_count += 1
            
            if removed_count > 0:
                logger.info(f"[ìºì‹œ ë™ê¸°í™”] ë©”ëª¨ë¦¬ì—ì„œ {removed_count}ê°œ í•­ëª© ì œê±°: {[f'{y}-{m:02d}' for y, m in to_remove]}")
            
            logger.debug(f"[ìºì‹œ ë™ê¸°í™”] DB ìºì‹œ: {len(db_cached_months)}ê°œ, ë©”ëª¨ë¦¬ ìºì‹œ: {len(self.event_cache)}ê°œ")
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬-DB ìºì‹œ ë™ê¸°í™” ì‹¤íŒ¨: {e}")

    def _check_cache_window_changed(self, center_year, center_month):
        """ìºì‹œ ìœˆë„ìš°ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        import datetime
        
        # ì˜¤ëŠ˜ ë‚ ì§œ
        today = datetime.date.today()
        today_key = (today.year, today.month)
        
        # í˜„ì¬ ìœˆë„ìš° ê³„ì‚° (7ê°œì›”: ì¤‘ì‹¬ì›” Â±3ê°œì›”)
        current_window = set(self._calculate_sliding_window(center_year, center_month, 3))
        
        # ì˜¤ëŠ˜ ë‚ ì§œê°€ ìœˆë„ìš°ì— ì—†ìœ¼ë©´ ê°•ì œë¡œ ì¶”ê°€ (í•­ìƒ ë³´í˜¸)
        if today_key not in current_window:
            current_window.add(today_key)
            logger.info(f"[ìºì‹œ ìœˆë„ìš°] ì˜¤ëŠ˜ ë‚ ì§œ {today.year}-{today.month:02d} ê°•ì œ ì¶”ê°€ (ì˜êµ¬ ë³´í˜¸)")
        
        # ìœˆë„ìš° ë³€í™” í™•ì¸
        window_changed = current_window != self.last_cache_window
        
        if window_changed:
            # ë³€í™” ë¡œê·¸
            added = current_window - self.last_cache_window
            removed = self.last_cache_window - current_window
            
            if added:
                added_str = [f"{y}-{m:02d}" for y, m in sorted(added)]
                logger.info(f"[ìºì‹œ ìœˆë„ìš°] ì¶”ê°€ëœ ì›”: {', '.join(added_str)}")
            
            if removed:
                removed_str = [f"{y}-{m:02d}" for y, m in sorted(removed)]
                logger.info(f"[ìºì‹œ ìœˆë„ìš°] ì œê±°ëœ ì›”: {', '.join(removed_str)}")
            
            # ìœˆë„ìš° ì—…ë°ì´íŠ¸
            self.last_cache_window = current_window.copy()
            logger.info(f"[ìºì‹œ ìœˆë„ìš°] ë³€í™” ê°ì§€: ì¤‘ì‹¬ì›” {center_year}-{center_month:02d}, ì´ {len(current_window)}ê°œì›”")
        else:
            logger.debug(f"[ìºì‹œ ìœˆë„ìš°] ë³€í™” ì—†ìŒ: ì¤‘ì‹¬ì›” {center_year}-{center_month:02d}")
        
        return window_changed, current_window

    def _calculate_sliding_window(self, center_year, center_month, radius):
        """ì¤‘ì‹¬ ì›” ê¸°ì¤€ìœ¼ë¡œ Â±radius ê°œì›”ì˜ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê³„ì‚° (ì¤‘ì‹¬ì›” í¬í•¨)"""
        from dateutil.relativedelta import relativedelta
        
        months = []
        center_date = datetime.date(center_year, center_month, 1)
        
        for i in range(-radius, radius + 1):
            # ì¤‘ì‹¬ì›”ë„ í¬í•¨í•˜ì—¬ ìœˆë„ìš° êµ¬ì„±
            target_date = center_date + relativedelta(months=i)
            months.append((target_date.year, target_date.month))
        
        return months

    def _fetch_events_from_providers(self, year, month):
        all_events = []
        start_day_of_week = self.settings.get("start_day_of_week", 6)
        start_date, end_date = get_month_view_dates(year, month, start_day_of_week)
        
        all_calendars = self.get_all_calendars(fetch_if_empty=False)
        custom_colors = self.settings.get("calendar_colors", {})
        default_color_map = {cal['id']: cal.get('backgroundColor', DEFAULT_EVENT_COLOR) for cal in all_calendars}
        for provider in self.providers:
            try:
                # data_manager ìì‹ ì„ providerì— ë„˜ê²¨ì£¼ì–´ ì˜¤ë¥˜ ë³´ê³ ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨
                events = provider.get_events(start_date, end_date, self)
                if events is not None:
                    filtered_events = []
                    for event in events:
                        event_id = event.get('id')
                        
                        # [ì¶”ê°€] ì‚­ì œ ëŒ€ê¸° ì¤‘ì¸ ì´ë²¤íŠ¸ëŠ” providerì—ì„œ ê°€ì ¸ì™€ë„ ë¬´ì‹œ
                        if event_id in self.pending_deletion_ids:
                            logger.debug(f"Filtered out pending deletion event: {event_id}")
                            continue
                        
                        # ë°˜ë³µì¼ì •ì¸ ê²½ìš° ë§ˆìŠ¤í„° IDë„ í™•ì¸
                        recurring_event_id = event.get('recurringEventId')
                        if recurring_event_id and recurring_event_id in self.pending_deletion_ids:
                            logger.debug(f"Filtered out recurring instance of pending deletion: {recurring_event_id}")
                            continue
                        
                        cal_id = event.get('calendarId')
                        if cal_id:
                            default_color = default_color_map.get(cal_id, DEFAULT_EVENT_COLOR)
                            event['color'] = custom_colors.get(cal_id, default_color)
                        filtered_events.append(event)
                    
                    all_events.extend(filtered_events)
            except Exception as e:
                msg = f"'{type(provider).__name__}'ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                logger.error(msg, exc_info=True)
                self.report_error(f"{msg}\n{e}")
        return all_events

    # [ì‚­ì œ] _run_immediate_sync, _on_immediate_sync_finished, _on_immediate_data_fetched ë©”ì„œë“œ ì‚­ì œ

    def get_events(self, year, month):
        cache_key = (year, month)
        self.last_requested_month = cache_key
        
        # UIì— ë‚ ì§œ ë³€ê²½ ì•Œë¦¼ (P1 ì‘ì—… ìš”ì²­)
        self.notify_date_changed(datetime.date(year, month, 1))
        
        # ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
        return self.event_cache.get(cache_key, [])

    def force_sync_month(self, year, month):
        logger.info(f"í˜„ì¬ ë³´ì´ëŠ” ì›”({year}ë…„ {month}ì›”)ì„ ê°•ì œë¡œ ì¦‰ì‹œ ë™ê¸°í™”í•©ë‹ˆë‹¤...")
        self.caching_manager.request_caching_around(year, month)

    def get_events_for_period(self, start_date, end_date):
        all_events = []
        months_to_check = set()
        current_date = start_date
        while current_date <= end_date:
            months_to_check.add((current_date.year, current_date.month))
            if current_date.month == 12:
                current_date = current_date.replace(year=current_date.year + 1, month=1, day=1)
            else:
                current_date = current_date.replace(month=current_date.month + 1, day=1)
        for year, month in months_to_check:
            # [ìˆ˜ì •] get_eventsëŠ” ì´ì œ ë¹„ë™ê¸° ìš”ì²­ë§Œ íŠ¸ë¦¬ê±°í•˜ë¯€ë¡œ, ì§ì ‘ ìºì‹œë¥¼ í™•ì¸
            monthly_events = self.event_cache.get((year, month), [])
            for event in monthly_events:
                try:
                    start_info = event.get('start', {})
                    end_info = event.get('end', {})
                    start_str = start_info.get('date') or start_info.get('dateTime', '')[:10]
                    end_str = end_info.get('date') or end_info.get('dateTime', '')[:10]
                    event_start_date = datetime.date.fromisoformat(start_str)
                    event_end_date = datetime.date.fromisoformat(end_str)
                    if 'date' in end_info:
                        event_end_date -= datetime.timedelta(days=1)
                    if not (event_end_date < start_date or event_start_date > end_date):
                        # [ì¶”ê°€] ì‚­ì œ ëŒ€ê¸° ì¤‘ì¸ ì´ë²¤íŠ¸ëŠ” ì œì™¸
                        event_id = event.get('id')
                        if event_id not in self.pending_deletion_ids:
                            all_events.append(event)
                        else:
                            logger.info(f"[DELETE DEBUG] Filtered out pending deletion event: {event_id}")
                except (ValueError, TypeError) as e:
                    logger.warning(f"ì´ë²¤íŠ¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}, ì´ë²¤íŠ¸: {event.get('summary')}")
                    continue
        unique_events = {e['id']: e for e in all_events}.values()
        return list(unique_events)

    def _on_calendar_thread_finished(self):
        if self.calendar_fetch_thread is None: return
            
        self.calendar_fetch_thread.quit()
        if self.calendar_fetch_thread.isRunning():
            self.calendar_fetch_thread.wait()
            
        self.calendar_fetcher.deleteLater()
        self.calendar_fetch_thread.deleteLater()
        self.calendar_fetch_thread = None
        self.calendar_fetcher = None
        logger.info("ìº˜ë¦°ë” ëª©ë¡ ìŠ¤ë ˆë“œ ì •ë¦¬ ì™„ë£Œ.")

    def _on_calendars_fetched(self, calendars):
        logger.info(f"{len(calendars)}ê°œì˜ ìº˜ë¦°ë” ëª©ë¡ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì‹ í–ˆìŠµë‹ˆë‹¤.")
        self.calendar_list_cache = calendars
        self._default_color_map_cache = None
        self.calendar_list_changed.emit()

    def add_event(self, event_data):
        """Enhanced Local-first event addition with recurring events support"""
        provider_name = event_data.get('provider')
        logger.info(f"Enhanced Local-first add_event ì‹œì‘: provider={provider_name}")
        
        # Provider ì¡´ì¬ í™•ì¸
        found_provider = None
        for provider in self.providers:
            if provider.name == provider_name:
                found_provider = provider
                break
        
        if not found_provider:
            logger.warning(f"Provider '{provider_name}' not found. Available providers: {[p.name for p in self.providers]}")
            return False
        
        # ë°˜ë³µ ì¼ì • í™•ì¸ ë° ì²˜ë¦¬
        event_body = event_data.get('body', {})
        recurrence = event_body.get('recurrence', [])
        
        if recurrence and len(recurrence) > 0:
            # ë°˜ë³µ ì¼ì •ì˜ Local-First ì²˜ë¦¬
            return self._add_recurring_event_local_first(event_data, recurrence[0])
        else:
            # ë‹¨ì¼ ì´ë²¤íŠ¸ì˜ ê¸°ì¡´ Local-First ì²˜ë¦¬
            return self._add_single_event_local_first(event_data)
        
    def _add_single_event_local_first(self, event_data):
        """ë‹¨ì¼ ì´ë²¤íŠ¸ì˜ Local-First ì¶”ê°€ (ê¸°ì¡´ ë¡œì§)"""
        # 1. ì¦‰ì‹œ optimistic event ìƒì„±
        optimistic_event = self._create_optimistic_event(event_data)
        logger.info(f"Single optimistic event ìƒì„±: id={optimistic_event.get('id')}")
        
        # 2. ì¦‰ì‹œ ë¡œì»¬ ìºì‹œ ì—…ë°ì´íŠ¸
        event_date = self._get_event_date(optimistic_event)
        self._update_cache_immediately(optimistic_event, event_date)
        logger.info(f"ë¡œì»¬ ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {event_date}")
        
        # 3. ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸ ë°œì‹ 
        self.data_updated.emit(event_date.year, event_date.month)
        logger.info(f"UI ì—…ë°ì´íŠ¸ ì‹œê·¸ë„ ë°œì‹ : {event_date.year}-{event_date.month}")
        
        # 4. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ë™ê¸°í™”
        self._queue_remote_add_event(event_data, optimistic_event)
        
        return True
    
    def _add_recurring_event_local_first(self, event_data, rrule_string):
        """ë°˜ë³µ ì¼ì •ì˜ Local-First ì¶”ê°€ - ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì¦‰ì‹œ ìƒì„±"""
        try:
            from rrule_parser import RRuleParser
            from dateutil import parser as dateutil_parser
            
            event_body = event_data.get('body', {})
            
            # ì‹œì‘ ë‚ ì§œ íŒŒì‹±
            start_info = event_body.get('start', {})
            if 'dateTime' in start_info:
                start_datetime = dateutil_parser.parse(start_info['dateTime'])
            elif 'date' in start_info:
                date_str = start_info['date']
                start_datetime = datetime.datetime.strptime(date_str, '%Y-%m-%d')
            else:
                logger.error("Invalid start datetime in recurring event")
                return False
            
            # RRULE íŒŒì‹±í•˜ì—¬ ëª¨ë“  ë°˜ë³µ ë‚ ì§œ ê³„ì‚°
            rrule_parser = RRuleParser()
            recurring_dates = rrule_parser.parse_google_rrule(
                rrule_string, start_datetime, max_instances=50
            )
            
            logger.info(f"Recurring event: generated {len(recurring_dates)} instances")
            
            # ëª¨ë“  ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¦‰ì‹œ ìºì‹œì— ì¶”ê°€
            affected_months = set()
            instances_added = 0
            
            # ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‚¬ìš©í•  ê³µí†µ base_id ìƒì„±
            import uuid
            base_id = event_body.get('id', 'unknown')
            if base_id == 'unknown':
                base_id = uuid.uuid4().hex[:8]
            
            logger.info(f"[RECURRING CREATE] Using common base_id: {base_id} for {len(recurring_dates)} instances")
            
            for i, recurring_date in enumerate(recurring_dates):
                # ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê³µí†µ base_id ì‚¬ìš©)
                instance = self._create_recurring_instance(event_body, recurring_date, i, base_id)
                
                # Optimistic ì´ë²¤íŠ¸ë¡œ ë³€í™˜
                optimistic_instance = self._create_optimistic_event_from_instance(event_data, instance)
                
                # ìºì‹œì— ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                instance_date = self._get_event_date(optimistic_instance)
                if instance_date:
                    cache_key = (instance_date.year, instance_date.month)
                    affected_months.add(cache_key)
                    
                    if cache_key not in self.event_cache:
                        self.event_cache[cache_key] = []
                    
                    # ì¤‘ë³µ ì²´í¬: ë™ì¼í•œ IDì˜ ì´ë²¤íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    instance_id = optimistic_instance.get('id')
                    existing_event = next((e for e in self.event_cache[cache_key] if e.get('id') == instance_id), None)
                    
                    if not existing_event:
                        self.event_cache[cache_key].append(optimistic_instance)
                        instances_added += 1
                    else:
                        logger.warning(f"Duplicate recurring instance prevented: {instance_id}")
            
            # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ ì¤‘ë³µ ì •ë¦¬ ë° UI ì—…ë°ì´íŠ¸
            for year, month in affected_months:
                cache_key = (year, month)
                self._cleanup_duplicate_events(cache_key)
                self.data_updated.emit(year, month)
                logger.info(f"UI ì—…ë°ì´íŠ¸ ì‹œê·¸ë„ ë°œì‹ : {year}-{month}")
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ë™ê¸°í™” (ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©)
            if instances_added > 0:
                # ì²« ë²ˆì§¸ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ì›ê²© ë™ê¸°í™”ì— ì‚¬ìš©
                first_instance = None
                for cache_key, events in self.event_cache.items():
                    for event in events:
                        if (event.get('_is_recurring_instance') and 
                            event.get('_instance_index') == 0 and
                            event.get('_sync_state') == 'pending'):
                            first_instance = event
                            break
                    if first_instance:
                        break
                
                if first_instance:
                    # ë°˜ë³µì¼ì •ì„ì„ í‘œì‹œí•˜ì—¬ íŠ¹ë³„í•œ ì²˜ë¦¬
                    first_instance['_is_recurring_master'] = True
                    self._queue_remote_add_recurring_event(event_data, first_instance)
            
            logger.info(f"Successfully added {instances_added} recurring instances to cache")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add recurring event: {e}")
            return False
    
    def _create_recurring_instance(self, base_event, recurring_date, index, base_id=None):
        """ê¸°ë³¸ ì´ë²¤íŠ¸ì—ì„œ ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        import uuid
        
        instance = base_event.copy()
        
        # ê³ ìœ  ID ìƒì„± (ì•ˆì „í•˜ê²Œ) - base_idê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ìƒì„±
        if base_id is None:
            base_id = base_event.get('id', 'unknown')
            if base_id == 'unknown':
                base_id = uuid.uuid4().hex[:8]
        instance['id'] = f"temp_recurring_{base_id}_{index}"
        
        # ì›ë³¸ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ íŒŒì‹±
        original_start = self._parse_event_datetime(base_event.get('start', {}))
        original_end = self._parse_event_datetime(base_event.get('end', {}))
        
        if original_start and original_end:
            # ì´ë²¤íŠ¸ ì§€ì† ì‹œê°„ ê³„ì‚°
            duration = original_end - original_start
            
            # ìƒˆë¡œìš´ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
            new_start = recurring_date.replace(
                hour=original_start.hour,
                minute=original_start.minute,
                second=original_start.second,
                microsecond=original_start.microsecond
            )
            new_end = new_start + duration
            
            # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì„¤ì •
            instance['start'] = self._format_event_datetime(new_start, base_event.get('start', {}))
            instance['end'] = self._format_event_datetime(new_end, base_event.get('end', {}))
        
        # ë°˜ë³µ ì •ë³´ ì„¤ì •
        instance['_is_recurring_instance'] = True
        instance['_instance_index'] = index
        instance['_master_event_id'] = base_id
        
        return instance
    
    def _parse_event_datetime(self, datetime_obj):
        """ì´ë²¤íŠ¸ì˜ ë‚ ì§œ/ì‹œê°„ íŒŒì‹±"""
        try:
            from dateutil import parser as dateutil_parser
            if 'dateTime' in datetime_obj:
                return dateutil_parser.parse(datetime_obj['dateTime'])
            elif 'date' in datetime_obj:
                date_str = datetime_obj['date']
                return datetime.datetime.strptime(date_str, '%Y-%m-%d')
        except Exception as e:
            logger.warning(f"Failed to parse event datetime: {e}")
        return None
    
    def _format_event_datetime(self, dt, original_format):
        """ë‚ ì§œ/ì‹œê°„ì„ ì´ë²¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
        if 'dateTime' in original_format:
            # ì‹œê°„ í¬í•¨ í˜•ì‹
            return {
                'dateTime': dt.strftime('%Y-%m-%dT%H:%M:%S'),
                'timeZone': original_format.get('timeZone', 'Asia/Seoul')
            }
        else:
            # ë‚ ì§œë§Œ í˜•ì‹ (ì¢…ì¼ ì´ë²¤íŠ¸)
            return {
                'date': dt.strftime('%Y-%m-%d')
            }
    
    def _create_optimistic_event_from_instance(self, event_data, instance):
        """ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ Optimistic ì´ë²¤íŠ¸ ìƒì„±"""
        import uuid
        
        optimistic_event = instance.copy()
        
        # IDê°€ ì—†ëŠ” ê²½ìš° ì•ˆì „í•˜ê²Œ ìƒì„±
        if 'id' not in optimistic_event or not optimistic_event['id']:
            base_id = event_data.get('body', {}).get('id', 'unknown')
            index = optimistic_event.get('_instance_index', 0)
            optimistic_event['id'] = f"temp_recurring_{base_id}_{index}"
        
        # ê¸°ë³¸ ì„¤ì •ë“¤
        optimistic_event['calendarId'] = event_data.get('calendarId')
        optimistic_event['provider'] = event_data.get('provider')
        optimistic_event['_sync_state'] = 'pending'
        
        # ìƒ‰ìƒ ì„¤ì •
        cal_id = optimistic_event.get('calendarId')
        if cal_id:
            target_color = self._get_calendar_color(cal_id)
            if target_color:
                optimistic_event['color'] = target_color
        
        return optimistic_event
        
    def _create_optimistic_event(self, event_data):
        """ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì„ì‹œ ì´ë²¤íŠ¸ ìƒì„±"""
        import uuid
        
        event_body = event_data.get('body', {})
        optimistic_event = event_body.copy()
        
        # ì„ì‹œ ID ìƒì„± (ì‹¤ì œ API IDë¡œ ë‚˜ì¤‘ì— êµì²´ë¨)
        if 'id' not in optimistic_event:
            optimistic_event['id'] = f"temp_{uuid.uuid4().hex[:8]}"
        
        # í•„ìˆ˜ í•„ë“œë“¤ ì„¤ì •
        optimistic_event['calendarId'] = event_data.get('calendarId')
        optimistic_event['provider'] = event_data.get('provider')
        optimistic_event['_sync_state'] = 'pending'  # ë™ê¸°í™” ìƒíƒœ ì¶”ê°€
        
        # ìƒ‰ìƒ ì •ë³´ ì„¤ì •
        cal_id = optimistic_event.get('calendarId')
        if cal_id:
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            cal_info = next((c for c in all_calendars if c['id'] == cal_id), None)
            default_color = cal_info.get('backgroundColor') if cal_info else DEFAULT_EVENT_COLOR
            optimistic_event['color'] = self.settings.get("calendar_colors", {}).get(cal_id, default_color)
        
        return optimistic_event
    
    def _get_event_date(self, event):
        """ì´ë²¤íŠ¸ì˜ ë‚ ì§œ ì¶”ì¶œ"""
        start_str = event['start'].get('date') or event['start'].get('dateTime')[:10]
        return datetime.date.fromisoformat(start_str)
    
    def _update_cache_immediately(self, event, event_date):
        """ì¦‰ì‹œ ë¡œì»¬ ìºì‹œ ì—…ë°ì´íŠ¸"""
        cache_key = (event_date.year, event_date.month)
        if cache_key in self.event_cache:
            self.event_cache[cache_key].append(event)
        else:
            self.event_cache[cache_key] = [event]
    
    def _queue_remote_add_event(self, event_data, optimistic_event):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ë™ê¸°í™” ì‹¤í–‰"""
        class RemoteAddTask(QRunnable):
            def __init__(self, data_manager, event_data, optimistic_event):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.optimistic_event = optimistic_event
                
            def run(self):
                provider_name = self.event_data.get('provider')
                temp_id = self.optimistic_event.get('id')
                
                # IDê°€ ì—†ìœ¼ë©´ ë™ê¸°í™”ë¥¼ ê±´ë„ˆë›°ê³  ë¡œê·¸ë§Œ ì¶œë ¥
                if not temp_id:
                    logger.warning(f"Optimistic event has no ID, skipping remote sync")
                    return
                
                for provider in self.data_manager.providers:
                    if provider.name == provider_name:
                        try:
                            logger.info(f"ì›ê²© API í˜¸ì¶œ ì‹œì‘: provider={provider_name}, temp_id={temp_id}")
                            # ì‹¤ì œ ì›ê²© API í˜¸ì¶œ
                            real_event = provider.add_event(self.event_data, self.data_manager)
                            if real_event:
                                logger.info(f"ì›ê²© API í˜¸ì¶œ ì„±ê³µ: real_id={real_event.get('id')}")
                                # ì„±ê³µ: ì„ì‹œ ì´ë²¤íŠ¸ë¥¼ ì‹¤ì œ ì´ë²¤íŠ¸ë¡œ êµì²´
                                self.data_manager._replace_optimistic_event(temp_id, real_event, provider_name)
                            else:
                                logger.warning(f"ì›ê²© API í˜¸ì¶œ ì‹¤íŒ¨: provider.add_event returned None")
                                # ì‹¤íŒ¨: ë™ê¸°í™” ì‹¤íŒ¨ ìƒíƒœë¡œ ë§ˆí¬
                                self.data_manager._mark_event_sync_failed(temp_id, "ì›ê²© ì¶”ê°€ ì‹¤íŒ¨")
                        except Exception as e:
                            logger.error(f"ì›ê²© ì´ë²¤íŠ¸ ì¶”ê°€ ì˜ˆì™¸ ë°œìƒ: {e}")
                            self.data_manager._mark_event_sync_failed(temp_id, str(e))
                        break
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        task = RemoteAddTask(self, event_data, optimistic_event)
        QThreadPool.globalInstance().start(task)
    
    def _queue_remote_add_recurring_event(self, event_data, first_instance):
        """ë°˜ë³µì¼ì • ì „ìš© ë°±ê·¸ë¼ìš´ë“œ ì›ê²© ë™ê¸°í™”"""
        class RemoteRecurringAddTask(QRunnable):
            def __init__(self, data_manager, event_data, first_instance):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.first_instance = first_instance
                
            def run(self):
                provider_name = self.event_data.get('provider')
                temp_id = self.first_instance.get('id')
                
                if not temp_id:
                    logger.warning(f"Recurring event has no ID, skipping remote sync")
                    return
                
                for provider in self.data_manager.providers:
                    if provider.name == provider_name:
                        try:
                            logger.info(f"ì›ê²© ë°˜ë³µì¼ì • API í˜¸ì¶œ ì‹œì‘: provider={provider_name}, temp_id={temp_id}")
                            # ì‹¤ì œ ì›ê²© API í˜¸ì¶œ
                            real_event = provider.add_event(self.event_data, self.data_manager)
                            if real_event:
                                logger.debug(f"Recurring sync API call success: real_id={real_event.get('id')}")
                                logger.info(f"ì›ê²© ë°˜ë³µì¼ì • API í˜¸ì¶œ ì„±ê³µ: real_id={real_event.get('id')}")
                                logger.debug("Calling _replace_optimistic_recurring_events")
                                # ì„±ê³µ: ëª¨ë“  ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹¤ì œ ì´ë²¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ êµì²´
                                self.data_manager._replace_optimistic_recurring_events(temp_id, real_event, provider_name)
                                logger.debug("_replace_optimistic_recurring_events completed")
                            else:
                                logger.warning(f"ì›ê²© ë°˜ë³µì¼ì • API í˜¸ì¶œ ì‹¤íŒ¨: provider.add_event returned None")
                                # ì‹¤íŒ¨: ë™ê¸°í™” ì‹¤íŒ¨ ìƒíƒœë¡œ ë§ˆí¬
                                self.data_manager._mark_recurring_events_sync_failed(temp_id, "ì›ê²© ì¶”ê°€ ì‹¤íŒ¨")
                        except Exception as e:
                            logger.error(f"ì›ê²© ë°˜ë³µì¼ì • ì¶”ê°€ ì˜ˆì™¸ ë°œìƒ: {e}")
                            self.data_manager._mark_recurring_events_sync_failed(temp_id, str(e))
                        break
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        task = RemoteRecurringAddTask(self, event_data, first_instance)
        QThreadPool.globalInstance().start(task)
    
    def _replace_optimistic_event(self, temp_id, real_event, provider_name):
        """ì„ì‹œ ì´ë²¤íŠ¸ë¥¼ ì‹¤ì œ ì´ë²¤íŠ¸ë¡œ êµì²´"""
        logger.info(f"ì„ì‹œ ì´ë²¤íŠ¸ êµì²´ ì‹œì‘: temp_id={temp_id}, real_id={real_event.get('id')}")
        
        if 'provider' not in real_event:
            real_event['provider'] = provider_name
        
        # ìƒ‰ìƒ ì •ë³´ ì¶”ê°€
        cal_id = real_event.get('calendarId')
        if cal_id:
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            cal_info = next((c for c in all_calendars if c['id'] == cal_id), None)
            default_color = cal_info.get('backgroundColor') if cal_info else DEFAULT_EVENT_COLOR
            real_event['color'] = self.settings.get("calendar_colors", {}).get(cal_id, default_color)
        
        real_event['_sync_state'] = 'synced'
        
        # ìºì‹œì—ì„œ ì„ì‹œ ì´ë²¤íŠ¸ ì°¾ì•„ì„œ êµì²´
        event_date = self._get_event_date(real_event)
        cache_key = (event_date.year, event_date.month)
        
        found_and_replaced = False
        if cache_key in self.event_cache:
            events = self.event_cache[cache_key]
            for i, event in enumerate(events):
                if event.get('id') == temp_id:
                    events[i] = real_event
                    found_and_replaced = True
                    logger.info(f"ì„ì‹œ ì´ë²¤íŠ¸ êµì²´ ì™„ë£Œ: {temp_id} -> {real_event.get('id')}")
                    break
        
        if not found_and_replaced:
            logger.warning(f"ì„ì‹œ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: temp_id={temp_id}, cache_key={cache_key}")
            # ì„ì‹œ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ì¶”ê°€
            if cache_key in self.event_cache:
                self.event_cache[cache_key].append(real_event)
            else:
                self.event_cache[cache_key] = [real_event]
            logger.info(f"ì‹¤ì œ ì´ë²¤íŠ¸ë¥¼ ìƒˆë¡œ ì¶”ê°€í•¨: {real_event.get('id')}")
        
        # UI ì—…ë°ì´íŠ¸
        self.data_updated.emit(event_date.year, event_date.month)
    
    def _replace_optimistic_recurring_events(self, temp_id, real_master_event, provider_name):
        """ëª¨ë“  ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹¤ì œ ì´ë²¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ êµì²´"""
        logger.info(f"[REPLACE DEBUG] ë°˜ë³µì¼ì • êµì²´ ì‹œì‘: temp_id={temp_id}, real_id={real_master_event.get('id')}")
        
        # ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ì¶”ì¶œ (temp_recurring_BASE_INDEXì—ì„œ BASE ë¶€ë¶„)
        master_event_id = self._extract_master_event_id(temp_id)
        logger.info(f"[REPLACE DEBUG] Extracted master_event_id: {master_event_id}")
        
        # ì‹¤ì œ ì´ë²¤íŠ¸ì—ì„œ ë°˜ë³µ ì •ë³´ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í•„ìš”
        # í•˜ì§€ë§Œ Googleì€ ë‹¨ì¼ ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ, 
        # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ì‹¤ì œ ì´ë²¤íŠ¸ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•¨
        
        affected_months = set()
        instances_replaced = 0
        
        for cache_key, events in self.event_cache.items():
            updated_events = []
            logger.info(f"[REPLACE DEBUG] Checking cache_key {cache_key} with {len(events)} events")
            
            for event in events:
                event_id = event.get('id', 'NO_ID')
                is_related = self._is_related_recurring_event(event, master_event_id)
                logger.debug(f"[REPLACE DEBUG] Event {event_id}: is_related={is_related}")
                
                if is_related:
                    # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹¤ì œ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    logger.info(f"[REPLACE DEBUG] Updating temp event: {event_id}")
                    updated_event = self._update_recurring_instance_with_real_data(event, real_master_event, provider_name)
                    updated_events.append(updated_event)
                    instances_replaced += 1
                    affected_months.add(cache_key)
                else:
                    updated_events.append(event)
            
            self.event_cache[cache_key] = updated_events
        
        # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ í•œ ë²ˆì— UI ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)
        if affected_months:
            # ëª¨ë“  ì›”ì„ ë™ì‹œì— ì—…ë°ì´íŠ¸ (ê¹œë¹¡ì„ ë°©ì§€)
            from PyQt6.QtCore import QTimer
            def update_all_affected_months():
                for year, month in affected_months:
                    self.data_updated.emit(year, month)
                logger.info(f"[REPLACE DEBUG] UI updated for all affected months: {list(affected_months)}")
            
            QTimer.singleShot(10, update_all_affected_months)  # ë” ì§§ì€ ì§€ì—°ìœ¼ë¡œ ë¹ ë¥¸ ì—…ë°ì´íŠ¸
        
        logger.info(f"[REPLACE DEBUG] ë°˜ë³µì¼ì • êµì²´ ì™„ë£Œ: {instances_replaced}ê°œ ì¸ìŠ¤í„´ìŠ¤ ì—…ë°ì´íŠ¸")
        logger.info(f"[REPLACE DEBUG] Affected months: {affected_months}")
        
        # êµì²´ í›„ ìºì‹œ ìƒíƒœ í™•ì¸
        for cache_key in affected_months:
            events_in_cache = len(self.event_cache.get(cache_key, []))
            logger.debug(f"[REPLACE DEBUG] After replacement, cache {cache_key} has {events_in_cache} events")
    
    def _update_recurring_instance_with_real_data(self, temp_instance, real_master_event, provider_name):
        """ì„ì‹œ ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹¤ì œ ì´ë²¤íŠ¸ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸"""
        updated_instance = temp_instance.copy()
        
        # ì„ì‹œ IDë¥¼ ì‹¤ì œ Google ì¸ìŠ¤í„´ìŠ¤ ID í˜•íƒœë¡œ ë³€ê²½
        temp_id = temp_instance.get('id', '')
        master_id = real_master_event.get('id')
        
        # Google Calendar ì¸ìŠ¤í„´ìŠ¤ ID í˜•íƒœë¡œ ë³€ê²½
        if temp_id.startswith('temp_recurring_') and master_id:
            # ì¸ìŠ¤í„´ìŠ¤ì˜ ì‹œì‘ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ Google í˜•íƒœì˜ ID ìƒì„±
            start_time = temp_instance.get('start', {})
            if start_time.get('dateTime'):
                # ë‚ ì§œì‹œê°„ì—ì„œ Google í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: 20250803T080000Z)
                import datetime
                try:
                    dt = datetime.datetime.fromisoformat(start_time['dateTime'].replace('Z', '+00:00'))
                    time_suffix = dt.strftime('%Y%m%dT%H%M%SZ')
                    updated_instance['id'] = f"{master_id}_{time_suffix}"
                    logger.debug(f"[UPDATE DEBUG] Changed temp ID {temp_id} to real ID {updated_instance['id']}")
                except:
                    # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë§ˆìŠ¤í„° IDë§Œ ì‚¬ìš©
                    updated_instance['id'] = master_id
                    logger.debug(f"[UPDATE DEBUG] Time parse failed, using master ID: {master_id}")
            else:
                # ì „ì¼ ì´ë²¤íŠ¸ì¸ ê²½ìš°
                updated_instance['id'] = master_id
        
        # ì‹¤ì œ ì´ë²¤íŠ¸ì˜ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸ (ì‹œê°„ì€ ìœ ì§€)
        updated_instance.update({
            'provider': provider_name,
            '_sync_state': 'synced',
            'summary': real_master_event.get('summary', updated_instance.get('summary')),
            'description': real_master_event.get('description', updated_instance.get('description')),
            'location': real_master_event.get('location', updated_instance.get('location')),
        })
        
        # ìƒ‰ìƒ ì •ë³´ ì¶”ê°€
        cal_id = real_master_event.get('calendarId')
        if cal_id:
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            cal_info = next((c for c in all_calendars if c['id'] == cal_id), None)
            default_color = cal_info.get('backgroundColor') if cal_info else DEFAULT_EVENT_COLOR
            updated_instance['color'] = self.settings.get("calendar_colors", {}).get(cal_id, default_color)
        
        # ì‹¤ì œ Google ì´ë²¤íŠ¸ì™€ì˜ ì—°ê²° ì •ë³´ ì¶”ê°€
        updated_instance['_google_master_id'] = real_master_event.get('id')
        updated_instance['recurringEventId'] = real_master_event.get('id')  # Google í˜•ì‹ì— ë§ê²Œ ì¶”ê°€
        
        return updated_instance
    
    def _mark_recurring_events_sync_failed(self, temp_id, error_msg):
        """ëª¨ë“  ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ì˜ ë™ê¸°í™” ì‹¤íŒ¨ ë§ˆí¬"""
        logger.warning(f"ë°˜ë³µì¼ì • ë™ê¸°í™” ì‹¤íŒ¨: temp_id={temp_id}, error={error_msg}")
        
        master_event_id = self._extract_master_event_id(temp_id)
        affected_months = set()
        
        for cache_key, events in self.event_cache.items():
            for event in events:
                if self._is_related_recurring_event(event, master_event_id):
                    event['_sync_state'] = 'failed'
                    event['_sync_error'] = error_msg
                    affected_months.add(cache_key)
        
        # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ UI ì—…ë°ì´íŠ¸
        for year, month in affected_months:
            self.data_updated.emit(year, month)
    
    def _mark_event_sync_failed(self, temp_id, error_msg):
        """ì´ë²¤íŠ¸ ë™ê¸°í™” ì‹¤íŒ¨ ë§ˆí¬"""
        logger.warning(f"ì´ë²¤íŠ¸ ë™ê¸°í™” ì‹¤íŒ¨: temp_id={temp_id}, error={error_msg}")
        
        # ëª¨ë“  ìºì‹œì—ì„œ í•´ë‹¹ ì´ë²¤íŠ¸ ì°¾ì•„ì„œ ì‹¤íŒ¨ ìƒíƒœë¡œ ë§ˆí¬
        found_event = False
        for cache_key, events in self.event_cache.items():
            for event in events:
                if event.get('id') == temp_id:
                    event['_sync_state'] = 'failed'
                    event['_sync_error'] = error_msg
                    found_event = True
                    logger.info(f"ì„ì‹œ ì´ë²¤íŠ¸ ì‹¤íŒ¨ ìƒíƒœë¡œ ë§ˆí¬: {temp_id}")
                    # UI ì—…ë°ì´íŠ¸í•˜ì—¬ ì‹¤íŒ¨ ìƒíƒœ í‘œì‹œ
                    year, month = cache_key
                    self.data_updated.emit(year, month)
                    return
        
        if not found_event:
            logger.error(f"ì‹¤íŒ¨ ë§ˆí¬í•  ì„ì‹œ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: temp_id={temp_id}")
    
    def _merge_events_preserving_temp(self, year, month, new_events):
        """ì„ì‹œ ì´ë²¤íŠ¸ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ìƒˆë¡œìš´ ì´ë²¤íŠ¸ë¡œ ìºì‹œ ì—…ë°ì´íŠ¸ (ë°˜ë³µì¼ì • ì¤‘ë³µ ë°©ì§€)"""
        cache_key = (year, month)
        existing_events = self.event_cache.get(cache_key, [])
        
        # ê¸°ì¡´ ìºì‹œì—ì„œ ì„ì‹œ ì´ë²¤íŠ¸ì™€ ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ ì°¾ê¸°
        temp_events = []
        synced_temp_events = []  # ì´ë¯¸ ë™ê¸°í™”ëœ ì„ì‹œ ì´ë²¤íŠ¸ë“¤
        
        for event in existing_events:
            event_id = event.get('id', '')
            sync_state = event.get('_sync_state')
            
            # ì„ì‹œ ì´ë²¤íŠ¸ì´ê±°ë‚˜ ë™ê¸°í™” ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ëŠ” êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
            if 'temp_' in event_id:
                if sync_state == 'synced':
                    synced_temp_events.append(event)
                elif sync_state in ['pending', 'failed']:
                    temp_events.append(event)
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ì¤‘ì—ì„œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²ƒê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ ì¶”ê°€
        unique_new_events = []
        
        for new_event in new_events:
            is_duplicate = False
            google_id = new_event.get('id')
            
            # [ì¶”ê°€] ì‚­ì œ ëŒ€ê¸° ì¤‘ì¸ ì´ë²¤íŠ¸ëŠ” ìºì‹œì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            if google_id in self.pending_deletion_ids:
                logger.info(f"[DELETE DEBUG] Blocked pending deletion event from cache: {google_id}")
                continue
                
            # ë°˜ë³µ ì´ë²¤íŠ¸ì˜ ê²½ìš° ë§ˆìŠ¤í„° IDë„ í™•ì¸
            recurring_event_id = new_event.get('recurringEventId')
            if recurring_event_id and recurring_event_id in self.pending_deletion_ids:
                logger.info(f"[DELETE DEBUG] Blocked recurring instance of pending deletion master: {recurring_event_id}")
                continue
            
            # 1. ì´ë¯¸ ë™ê¸°í™”ëœ ì„ì‹œ ì´ë²¤íŠ¸ì™€ ì¤‘ë³µ í™•ì¸ (Google Master IDë¡œ)
            for synced_event in synced_temp_events:
                if synced_event.get('_google_master_id') == google_id:
                    is_duplicate = True
                    logger.debug(f"ì¤‘ë³µ ë°©ì§€: Google ì´ë²¤íŠ¸ {google_id}ëŠ” ì´ë¯¸ ë™ê¸°í™”ëœ ì„ì‹œ ì´ë²¤íŠ¸ ì¡´ì¬")
                    break
            
            # 2. ìƒˆë¡œìš´ ì´ë²¤íŠ¸ë“¤ ê°„ì˜ ì¤‘ë³µ í™•ì¸
            if not is_duplicate:
                for existing_new in unique_new_events:
                    if self._is_duplicate_event(new_event, existing_new):
                        is_duplicate = True
                        logger.debug(f"ì¤‘ë³µ ë°©ì§€: ìƒˆë¡œìš´ ì´ë²¤íŠ¸ë“¤ ê°„ ì¤‘ë³µ ë°œê²¬")
                        break
            
            # 3. ë°˜ë³µì¼ì •ì˜ ê²½ìš° ì œëª©, ì‹œê°„ ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (ì„ì‹œ ì´ë²¤íŠ¸ì™€)
            if not is_duplicate and new_event.get('recurringEventId'):
                for temp_event in temp_events:
                    if self._is_same_recurring_event(new_event, temp_event):
                        is_duplicate = True
                        logger.debug(f"ì¤‘ë³µ ë°©ì§€: ë°˜ë³µì¼ì • ë§¤ì¹­ë¨ (ì„ì‹œ) - {new_event.get('summary')}")
                        break
            
            # 4. ë°˜ë³µì¼ì •ì˜ ê²½ìš° ì´ë¯¸ ë™ê¸°í™”ëœ ì´ë²¤íŠ¸ì™€ì˜ ì¤‘ë³µ í™•ì¸
            if not is_duplicate and new_event.get('recurringEventId'):
                new_event_id = new_event.get('id')
                new_recurring_id = new_event.get('recurringEventId')
                
                for synced_event in synced_temp_events:
                    # ë™ì¼í•œ ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ì—ì„œ ë‚˜ì˜¨ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
                    synced_master_id = synced_event.get('_google_master_id')
                    synced_event_id = synced_event.get('id', '')
                    
                    if (synced_master_id == new_recurring_id or 
                        synced_event_id == new_event_id or
                        self._is_same_recurring_event(new_event, synced_event)):
                        is_duplicate = True
                        logger.debug(f"[MERGE DEBUG] ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ ë™ê¸°í™”ëœ ë°˜ë³µì¼ì •ê³¼ ë§¤ì¹­ë¨ - {new_event.get('summary')}")
                        break
            
            if not is_duplicate:
                unique_new_events.append(new_event)
        
        # ëª¨ë“  ì´ë²¤íŠ¸ í•©ì¹˜ê¸°: ê³ ìœ í•œ ìƒˆ ì´ë²¤íŠ¸ + ë™ê¸°í™”ëœ ì„ì‹œ ì´ë²¤íŠ¸ + ë³´ì¡´í•  ì„ì‹œ ì´ë²¤íŠ¸
        merged_events = unique_new_events + synced_temp_events + temp_events
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        self.event_cache[cache_key] = merged_events
        logger.info(f"ìºì‹œ ë³‘í•© ì™„ë£Œ: {year}-{month}, ì „ì²´={len(merged_events)}ê°œ "
                   f"(ìƒˆì´ë²¤íŠ¸={len(unique_new_events)}, ë™ê¸°í™”ì™„ë£Œ={len(synced_temp_events)}, ì„ì‹œë³´ì¡´={len(temp_events)})")
    
    def _is_duplicate_event(self, event1, event2):
        """ë‘ ì´ë²¤íŠ¸ê°€ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        # IDê°€ ë™ì¼í•œ ê²½ìš°
        if event1.get('id') == event2.get('id'):
            return True
        
        # ì œëª©, ì‹œì‘ì‹œê°„, ì¢…ë£Œì‹œê°„ì´ ëª¨ë‘ ë™ì¼í•œ ê²½ìš°
        if (event1.get('summary') == event2.get('summary') and
            event1.get('start') == event2.get('start') and
            event1.get('end') == event2.get('end')):
            return True
            
        return False
    
    def _is_same_recurring_event(self, google_event, temp_event):
        """Google ì´ë²¤íŠ¸ì™€ ì„ì‹œ ë°˜ë³µ ì´ë²¤íŠ¸ê°€ ê°™ì€ ì´ë²¤íŠ¸ì¸ì§€ í™•ì¸"""
        # 1. ì œëª©ì´ ê°™ê³ 
        if google_event.get('summary') != temp_event.get('summary'):
            return False
        
        # 2. ì‹œê°„ì´ ë¹„ìŠ·í•˜ê³  (ë°˜ë³µì¼ì •ì˜ ê° ì¸ìŠ¤í„´ìŠ¤)
        google_start = google_event.get('start', {})
        temp_start = temp_event.get('start', {})
        
        # ì‹œì‘ ì‹œê°„ ë¹„êµ (ë‚ ì§œëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ì‹œê°„ì€ ê°™ì•„ì•¼ í•¨)
        google_time = google_start.get('dateTime', google_start.get('date', ''))
        temp_time = temp_start.get('dateTime', temp_start.get('date', ''))
        
        if google_time and temp_time:
            try:
                # ì‹œê°„ ë¶€ë¶„ë§Œ ë¹„êµ (HH:MM)
                google_time_part = google_time.split('T')[1][:5] if 'T' in google_time else ''
                temp_time_part = temp_time.split('T')[1][:5] if 'T' in temp_time else ''
                
                if google_time_part == temp_time_part:
                    return True
            except:
                pass
        
        return False

    # Local-first delete_event method
    def delete_event(self, event_data, deletion_mode='all'):
        """Enhanced Local-first event deletion with recurring events support"""
        logger.info("delete_event called")
        event_body = event_data.get('body', event_data)
        event_id = event_body.get('id')
        
        logger.info(f"Deleting event: {event_id}, mode: {deletion_mode}")
        logger.info(f"[DELETE DEBUG] Enhanced Local-first delete_event: id={event_id}, mode={deletion_mode}")
        logger.info(f"[DELETE DEBUG] Event data: {event_data}")
        logger.info(f"[DELETE DEBUG] Event body keys: {list(event_body.keys())}")
        
        # ë°˜ë³µ ì¼ì •ì¸ì§€ í™•ì¸
        is_recurring = self._is_recurring_event(event_id)
        logger.info(f"[DELETE DEBUG] Is recurring: {is_recurring}")
        
        if is_recurring and deletion_mode == 'all':
            # ë°˜ë³µ ì¼ì • ì „ì²´ ì‚­ì œ
            return self._delete_all_recurring_instances(event_data)
        elif is_recurring and deletion_mode in ['instance', 'future']:
            # ë°˜ë³µ ì¼ì • ë¶€ë¶„ ì‚­ì œ - ì¶”í›„ êµ¬í˜„
            logger.warning(f"Recurring partial deletion ({deletion_mode}) not fully implemented yet")
            return self._delete_single_event_local_first(event_data, deletion_mode)
        else:
            # ë‹¨ì¼ ì´ë²¤íŠ¸ ì‚­ì œ
            return self._delete_single_event_local_first(event_data, deletion_mode)
    
    def _delete_single_event_local_first(self, event_data, deletion_mode):
        """ë‹¨ì¼ ì´ë²¤íŠ¸ì˜ Local-First ì‚­ì œ (ê¸°ì¡´ ë¡œì§)"""
        event_body = event_data.get('body', event_data)
        event_id = event_body.get('id')
        
        # 1. ì¦‰ì‹œ ë¡œì»¬ ìºì‹œì—ì„œ ì´ë²¤íŠ¸ ì°¾ê¸° ë° ë°±ì—…
        deleted_event, cache_key = self._find_and_backup_event(event_id)
        if not deleted_event:
            logger.warning(f"Event {event_id} not found in cache")
            return False
        
        # 2. ì¦‰ì‹œ ë¡œì»¬ ìºì‹œì—ì„œ ì´ë²¤íŠ¸ ì œê±°
        self._remove_event_from_cache(event_id)
        
        # 3. ì¦‰ì‹œ ì™„ë£Œ ìƒíƒœ ì •ë¦¬
        self.unmark_event_as_completed(event_id)
        
        # 4. [ì¶”ê°€] ì‚­ì œ ëŒ€ê¸° ëª©ë¡ì— ì¶”ê°€
        self.pending_deletion_ids.add(event_id)
        logger.info(f"[DELETE DEBUG] Added single event to pending deletion: {event_id}")
        
        # 5. ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸
        year, month = cache_key
        self.data_updated.emit(year, month)
        
        # 6. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ë™ê¸°í™”
        self._queue_remote_delete_event(event_data, deleted_event, deletion_mode)
        
        return True
    
    def _delete_all_recurring_instances(self, event_data):
        """ë°˜ë³µ ì¼ì •ì˜ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì¦‰ì‹œ ì‚­ì œ"""
        try:
            # [ì¶”ê°€] ë°°ì¹˜ ì‚­ì œ ëª¨ë“œ ì‹œì‘ - UI refresh ì°¨ë‹¨
            self.batch_deletion_mode = True
            logger.debug("Batch deletion mode started")
            logger.info(f"[DELETE DEBUG] Batch deletion mode started")
            
            event_body = event_data.get('body', event_data)
            event_id = event_body.get('id')
            
            logger.info(f"[DELETE DEBUG] _delete_all_recurring_instances called")
            logger.info(f"[DELETE DEBUG] Original event_id: {event_id}")
            logger.info(f"[DELETE DEBUG] Event body: {event_body}")
            
            # ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ì¶”ì¶œ
            master_event_id = self._extract_master_event_id(event_id)
            logger.info(f"[DELETE DEBUG] Extracted master_event_id: {master_event_id}")
            
            logger.info(f"Deleting all recurring instances for master_id: {master_event_id}")
            
            # ëª¨ë“  ìºì‹œì—ì„œ ê´€ë ¨ ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸° ë° ì‚­ì œ
            affected_months = set()
            instances_deleted = 0
            deleted_event_ids = []  # ì‚­ì œëœ ì´ë²¤íŠ¸ ID ì¶”ì 
            
            for cache_key, events in self.event_cache.items():
                # ì‚­ì œí•  ì´ë²¤íŠ¸ë“¤ ì°¾ê¸°
                events_to_remove = []
                logger.debug(f"Checking cache_key {cache_key} with {len(events)} events")
                logger.info(f"[DELETE DEBUG] Checking cache_key {cache_key} with {len(events)} events")
                
                for i, event in enumerate(events):
                    event_id = event.get('id', 'NO_ID')
                    sync_state = event.get('_sync_state', 'NO_STATE')
                    google_master_id = event.get('_google_master_id', 'NO_GOOGLE_ID')
                    
                    logger.info(f"[DELETE DEBUG] Event {i}: id={event_id}, sync_state={sync_state}, google_master_id={google_master_id}")
                    
                    is_related = self._is_related_recurring_event(event, master_event_id)
                    if is_related:
                        events_to_remove.append(event)
                        deleted_event_ids.append(event_id)  # ì‚­ì œëœ ID ê¸°ë¡
                        # ì™„ë£Œ ìƒíƒœë„ ì •ë¦¬
                        self.unmark_event_as_completed(event.get('id'))
                        logger.debug(f"Found recurring instance to delete: {event.get('id')}")
                        logger.debug(f"Event summary: {event.get('summary', 'NO_SUMMARY')}")
                        logger.info(f"[DELETE DEBUG] Found recurring instance to delete: {event.get('id')}")
                        logger.info(f"[DELETE DEBUG] Event summary: {event.get('summary', 'NO_SUMMARY')}")
                    else:
                        logger.debug(f"Event {event_id} not related to master {master_event_id}")
                        logger.debug(f"[DELETE DEBUG] Event {event_id} not related to master {master_event_id}")
                
                # ì´ë²¤íŠ¸ ì‚­ì œ (ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ)
                if events_to_remove:
                    original_count = len(events)
                    # ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¡œ êµì²´ (ì•ˆì „í•œ ì‚­ì œ)
                    remaining_events = []
                    for event in events:
                        if event not in events_to_remove:
                            remaining_events.append(event)
                    
                    self.event_cache[cache_key] = remaining_events
                    deleted_count = original_count - len(remaining_events)
                    instances_deleted += deleted_count
                    affected_months.add(cache_key)
                    
                    logger.debug(f"Deleted {deleted_count} events from cache_key {cache_key}")
                    logger.info(f"Deleted {deleted_count} events from cache_key {cache_key}")
            
            # [ì¶”ê°€] ì‚­ì œëœ ì´ë²¤íŠ¸ IDë“¤ì„ pending deletionìœ¼ë¡œ ì¶”ê°€
            # ë§ˆìŠ¤í„° IDì™€ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ID í¬í•¨
            self.pending_deletion_ids.add(master_event_id)
            for deleted_id in deleted_event_ids:
                self.pending_deletion_ids.add(deleted_id)
            logger.info(f"[DELETE DEBUG] Added to pending deletion: master={master_event_id}, instances={deleted_event_ids}")
            
            # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ ë™ì‹œì— UI ì—…ë°ì´íŠ¸ (ì§€ì—° ì—†ì´ í•œë²ˆì—)
            if affected_months:
                # QTimerë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ëª¨ë“  UI ì—…ë°ì´íŠ¸ë¥¼ í•œë²ˆì— ì²˜ë¦¬
                from PyQt6.QtCore import QTimer
                for year, month in affected_months:
                    QTimer.singleShot(0, lambda y=year, m=month: self.data_updated.emit(y, m))
                logger.info(f"UI ì—…ë°ì´íŠ¸ ì‹œê·¸ë„ ë°œì‹  (ë™ì‹œ): {list(affected_months)}")
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ ì‚­ì œ ì²˜ë¦¬
            logger.info(f"[DELETE DEBUG] Queuing remote delete for event_data: {event_data}")
            logger.info(f"[DELETE DEBUG] Queuing remote delete for event_body: {event_body}")
            self._queue_remote_delete_event(event_data, event_body, 'all')
            
            logger.info(f"Successfully deleted {instances_deleted} recurring instances from cache")
            return instances_deleted > 0
            
        except Exception as e:
            logger.error(f"Failed to delete recurring event: {e}")
            # [ì¶”ê°€] ì˜¤ë¥˜ ì‹œì—ë„ ë°°ì¹˜ ì‚­ì œ ëª¨ë“œ í•´ì œ
            self.batch_deletion_mode = False
            return False
    
    def _is_recurring_event(self, event_id):
        """ì´ë²¤íŠ¸ê°€ ë°˜ë³µ ì¼ì •ì¸ì§€ í™•ì¸"""
        if not event_id:
            return False
        
        # ì„ì‹œ ID íŒ¨í„´ í™•ì¸
        if event_id.startswith('temp_recurring_'):
            return True
        
        # ìºì‹œì—ì„œ ì´ë²¤íŠ¸ ì°¾ì•„ì„œ ë°˜ë³µ ì •ë³´ í™•ì¸
        for cache_key, events in self.event_cache.items():
            for event in events:
                if event.get('id') == event_id:
                    return (event.get('_is_recurring_instance', False) or 
                           'recurrence' in event or
                           event.get('recurringEventId') is not None)
        
        return False
    
    def _extract_master_event_id(self, event_id):
        """ì´ë²¤íŠ¸ IDì—ì„œ ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ì¶”ì¶œ"""
        if event_id and event_id.startswith('temp_recurring_'):
            # temp_recurring_base_id_indexì—ì„œ base_id ì¶”ì¶œ
            parts = event_id.split('_')
            if len(parts) >= 3:
                return '_'.join(parts[2:-1])  # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ ì œì™¸
        
        # Google ë°˜ë³µì¼ì • íŒ¨í„´ ì²˜ë¦¬ (masterid_timestamp í˜•íƒœ)
        if event_id and '_' in event_id and event_id.split('_')[-1].endswith('Z'):
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ë§ˆìŠ¤í„° ID ì¶”ì¶œ
            master_id = event_id.rsplit('_', 1)[0]
            logger.debug(f"Master event ID extracted: {event_id} -> {master_id}")
            return master_id
        
        return event_id
    
    def _is_related_recurring_event(self, event, master_event_id):
        """ì´ë²¤íŠ¸ê°€ íŠ¹ì • ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸ (ê°•í™”ëœ ë²„ì „)"""
        event_id = event.get('id', '')
        
        # 1. ì§ì ‘ì ì¸ ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ë§¤ì¹˜
        if event.get('_master_event_id') == master_event_id:
            return True
        
        # 2. ë™ì¼í•œ ID (ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ìì²´)
        if event.get('id') == master_event_id:
            return True
        
        # 3. ì„ì‹œ ë°˜ë³µ ID íŒ¨í„´ ë§¤ì¹˜
        if event_id.startswith('temp_recurring_'):
            # temp_recurring_BASE_INDEX í˜•íƒœì—ì„œ BASE ë¶€ë¶„ ì¶”ì¶œ
            parts = event_id.split('_')
            if len(parts) >= 3:
                # ë§ˆì§€ë§‰ ë¶€ë¶„(ì¸ë±ìŠ¤) ì œì™¸í•˜ê³  base ë¶€ë¶„ ì¶”ì¶œ
                event_base_id = '_'.join(parts[2:-1])
                if event_base_id == master_event_id:
                    return True
                # ë¶€ë¶„ ë§¤ì¹˜ë„ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
                if master_event_id in event_base_id:
                    return True
        
        # 4. Google ë°˜ë³µì¼ì • íŒ¨í„´ í™•ì¸ (masterid_timestamp í˜•íƒœ)
        if '_' in event_id and event_id.split('_')[-1].endswith('Z'):
            # Google ì¸ìŠ¤í„´ìŠ¤ IDì—ì„œ ë§ˆìŠ¤í„° ID ì¶”ì¶œ
            event_master_id = event_id.rsplit('_', 1)[0]
            if event_master_id == master_event_id:
                return True
                
        # 5. recurringEventId í™•ì¸
        if event.get('recurringEventId') == master_event_id:
            return True
        
        # 6. ë°˜ë³µ ì´ë²¤íŠ¸ íŠ¹ì„± í™•ì¸
        if event.get('_is_recurring_instance') and master_event_id in str(event_id):
            return True
            
        return False
    
    def _cleanup_duplicate_events(self, cache_key):
        """íŠ¹ì • ìºì‹œ í‚¤ì—ì„œ ì¤‘ë³µ ì´ë²¤íŠ¸ ì •ë¦¬"""
        if cache_key not in self.event_cache:
            return
            
        events = self.event_cache[cache_key]
        seen_ids = set()
        unique_events = []
        
        for event in events:
            event_id = event.get('id')
            if event_id and event_id not in seen_ids:
                seen_ids.add(event_id)
                unique_events.append(event)
            elif event_id:
                logger.warning(f"Removed duplicate event: {event_id}")
        
        if len(unique_events) != len(events):
            self.event_cache[cache_key] = unique_events
            logger.info(f"Cleaned up {len(events) - len(unique_events)} duplicate events in {cache_key}")
    
    def _remove_event_from_cache(self, event_id):
        """ìºì‹œì—ì„œ ì´ë²¤íŠ¸ ì œê±°"""
        for cache_key, events in self.event_cache.items():
            self.event_cache[cache_key] = [e for e in events if e.get('id') != event_id]
    
    def _queue_remote_delete_event(self, event_data, deleted_event, deletion_mode):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ì‚­ì œ ë™ê¸°í™”"""
        class RemoteDeleteTask(QRunnable):
            def __init__(self, data_manager, event_data, deleted_event, deletion_mode):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.deleted_event = deleted_event
                self.deletion_mode = deletion_mode
                
            def run(self):
                provider_name = self.event_data.get('provider')
                event_id = self.deleted_event.get('id', 'NO_ID')
                
                logger.info(f"[DELETE DEBUG] RemoteDeleteTask.run() called")
                logger.info(f"[DELETE DEBUG] Provider: {provider_name}")
                logger.info(f"[DELETE DEBUG] Event ID for deletion: {event_id}")
                logger.info(f"[DELETE DEBUG] Deleted event data: {self.deleted_event}")
                logger.info(f"[DELETE DEBUG] Event data for provider: {self.event_data}")
                logger.info(f"[DELETE DEBUG] Deletion mode: {self.deletion_mode}")
                
                # ì„ì‹œ ID ì²´í¬
                if event_id.startswith('temp_'):
                    logger.warning(f"[DELETE DEBUG] PROBLEM: Trying to delete with temporary ID: {event_id}")
                    # ì‹¤ì œ Google ID ì°¾ê¸° ì‹œë„
                    google_id = self.deleted_event.get('_google_master_id')
                    if google_id:
                        logger.info(f"[DELETE DEBUG] Found linked Google ID: {google_id}")
                        # ì‹¤ì œ Google IDë¡œ ì´ë²¤íŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸
                        updated_event_data = self.event_data.copy()
                        updated_event_data['body'] = self.deleted_event.copy()
                        updated_event_data['body']['id'] = google_id
                        logger.info(f"[DELETE DEBUG] Updated event data for deletion: {updated_event_data}")
                    else:
                        logger.error(f"[DELETE DEBUG] No Google ID found for temp event: {event_id}")
                        return
                
                for provider in self.data_manager.providers:
                    if provider.name == provider_name:
                        try:
                            # ì‹¤ì œ ì›ê²© API í˜¸ì¶œ
                            success = provider.delete_event(updated_event_data if 'updated_event_data' in locals() else self.event_data, 
                                                           data_manager=self.data_manager, deletion_mode=self.deletion_mode)
                            if success:
                                # ì„±ê³µ: ì‚­ì œ í™•ì¸ ë° pending deletionì—ì„œ ì œê±°
                                logger.info(f"ì´ë²¤íŠ¸ {event_id} ì›ê²© ì‚­ì œ ì„±ê³µ")
                                
                                # ë°˜ë³µì¼ì • ì „ì²´ ì‚­ì œì¸ ê²½ìš° ëª¨ë“  ê´€ë ¨ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•œë²ˆì— ì œê±°
                                # Google ë°˜ë³µì¼ì • íŒ¨í„´: masterid_timestamp í˜•íƒœ
                                has_timestamp_suffix = '_' in event_id and event_id.split('_')[-1].endswith('Z')
                                is_recurring = (event_id.startswith('temp_recurring_') or 
                                              self.deleted_event.get('recurringEventId') or
                                              has_timestamp_suffix)  # Google recurring event pattern
                                
                                logger.debug(f"Checking recurring status: mode={self.deletion_mode}, is_recurring={is_recurring}")
                                logger.debug(f"Event ID pattern: {event_id}")
                                logger.debug(f"Deleted event recurringEventId: {self.deleted_event.get('recurringEventId')}")
                                logger.debug(f"Has timestamp suffix: {has_timestamp_suffix}")
                                
                                if self.deletion_mode == 'all' and is_recurring:
                                    logger.debug("Calling _remove_all_recurring_from_pending_deletion")
                                    self.data_manager._remove_all_recurring_from_pending_deletion(event_id, self.deleted_event)
                                else:
                                    logger.debug("Calling _remove_from_pending_deletion (single)")
                                    self.data_manager._remove_from_pending_deletion(event_id)
                            else:
                                # ì‹¤íŒ¨: ì´ë²¤íŠ¸ë¥¼ ìºì‹œì— ë³µì›
                                self.data_manager._restore_deleted_event(self.deleted_event, "ì›ê²© ì‚­ì œ ì‹¤íŒ¨")
                        except Exception as e:
                            logger.error(f"ì›ê²© ì´ë²¤íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
                            self.data_manager._restore_deleted_event(self.deleted_event, str(e))
                        break
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        task = RemoteDeleteTask(self, event_data, deleted_event, deletion_mode)
        QThreadPool.globalInstance().start(task)
    
    def _restore_deleted_event(self, deleted_event, error_msg):
        """ì‚­ì œ ì‹¤íŒ¨ ì‹œ ì´ë²¤íŠ¸ ë³µì›"""
        event_id = deleted_event.get('id')
        event_summary = deleted_event.get('summary', 'No summary')
        print(f"DEBUG: _restore_deleted_event called for event: {event_summary} (ID: {event_id})")
        print(f"DEBUG: Error: {error_msg}")
        
        deleted_event['_sync_state'] = 'failed'
        deleted_event['_sync_error'] = error_msg
        
        # [ì¶”ê°€] ì‚­ì œ ì‹¤íŒ¨ ì‹œ pending deletionì—ì„œ ì œê±°
        self._remove_from_pending_deletion(event_id)
        
        # ì´ë²¤íŠ¸ ë‚ ì§œë¡œ ì ì ˆí•œ ìºì‹œì— ë³µì›
        event_date = self._get_event_date(deleted_event)
        cache_key = (event_date.year, event_date.month)
        
        if cache_key in self.event_cache:
            self.event_cache[cache_key].append(deleted_event)
        else:
            self.event_cache[cache_key] = [deleted_event]
        
        # UI ì—…ë°ì´íŠ¸í•˜ì—¬ ë³µì›ëœ ì´ë²¤íŠ¸ í‘œì‹œ
        self.data_updated.emit(event_date.year, event_date.month)
    
    def _remove_from_pending_deletion(self, event_id):
        """ì‚­ì œ ëŒ€ê¸° ëª©ë¡ì—ì„œ ì´ë²¤íŠ¸ ID ì œê±°"""
        if event_id in self.pending_deletion_ids:
            self.pending_deletion_ids.remove(event_id)
            logger.info(f"[DELETE DEBUG] Removed {event_id} from pending deletion")
        
        # ë§ˆìŠ¤í„° ID ê´€ë ¨ ëª¨ë“  IDë“¤ë„ ì œê±° (ë°˜ë³µ ì´ë²¤íŠ¸ ëŒ€ì‘)
        if event_id.startswith('temp_recurring_'):
            master_id = self._extract_master_event_id(event_id)
            self.pending_deletion_ids.discard(master_id)
            
            # ê´€ë ¨ëœ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ IDë“¤ë„ ì œê±°
            to_remove = []
            for pending_id in self.pending_deletion_ids:
                if (pending_id.startswith('temp_recurring_') and 
                    self._extract_master_event_id(pending_id) == master_id):
                    to_remove.append(pending_id)
            
            for remove_id in to_remove:
                self.pending_deletion_ids.discard(remove_id)
            
            logger.info(f"[DELETE DEBUG] Removed master {master_id} and {len(to_remove)} related instances from pending deletion")
    
    def _remove_all_recurring_from_pending_deletion(self, event_id, deleted_event):
        """ë°˜ë³µì¼ì •ì˜ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•œë²ˆì— pending deletionì—ì„œ ì œê±°í•˜ê³  UI ì—…ë°ì´íŠ¸"""
        logger.info(f"[DELETE DEBUG] _remove_all_recurring_from_pending_deletion called with batch_mode: {self.batch_deletion_mode}")
        
        try:
            # ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ì¶”ì¶œ
            if event_id.startswith('temp_recurring_'):
                master_id = self._extract_master_event_id(event_id)
            else:
                # Google IDì¸ ê²½ìš° - recurringEventId ë˜ëŠ” timestamp ì œê±°í•˜ì—¬ master ID ì¶”ì¶œ
                master_id = deleted_event.get('recurringEventId')
                if not master_id:
                    # Google ë°˜ë³µì¼ì • ì¸ìŠ¤í„´ìŠ¤ IDì—ì„œ ë§ˆìŠ¤í„° ID ì¶”ì¶œ (timestamp ë¶€ë¶„ ì œê±°)
                    if '_' in event_id and event_id.split('_')[-1].endswith('Z'):
                        master_id = event_id.rsplit('_', 1)[0]  # ë§ˆì§€ë§‰ _timestamp ë¶€ë¶„ ì œê±°
                    else:
                        master_id = event_id
            
            logger.info(f"[DELETE DEBUG] _remove_all_recurring_from_pending_deletion: master_id={master_id}")
            
            # ì œê±°ë  ëª¨ë“  IDë“¤ ìˆ˜ì§‘
            removed_ids = []
            affected_months = set()
            
            # ë§ˆìŠ¤í„° ID ì œê±°
            if master_id in self.pending_deletion_ids:
                self.pending_deletion_ids.remove(master_id)
                removed_ids.append(master_id)
            
            # ê´€ë ¨ëœ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ IDë“¤ ì œê±°
            to_remove = []
            for pending_id in list(self.pending_deletion_ids):  # ë³µì‚¬ë³¸ìœ¼ë¡œ ì•ˆì „í•œ ìˆœíšŒ
                if (pending_id.startswith('temp_recurring_') and 
                    self._extract_master_event_id(pending_id) == master_id):
                    to_remove.append(pending_id)
                    
                    # í•´ë‹¹ ì´ë²¤íŠ¸ê°€ ì–´ëŠ ì›”ì— ì†í•˜ëŠ”ì§€ í™•ì¸
                    for cache_key, events in self.event_cache.items():
                        for event in events:
                            if event.get('id') == pending_id:
                                affected_months.add(cache_key)
                                break
            
            # ì‹¤ì œ ì œê±°
            for remove_id in to_remove:
                self.pending_deletion_ids.discard(remove_id)
                removed_ids.append(remove_id)
            
            logger.info(f"[DELETE DEBUG] Removed all recurring instances: master={master_id}, instances={len(to_remove)}")
            logger.info(f"[DELETE DEBUG] Total removed IDs: {removed_ids}")
            logger.info(f"[DELETE DEBUG] Affected months: {affected_months}")
            
        finally:
            # ë°°ì¹˜ ì‚­ì œ ëª¨ë“œ ë¹„í™œì„±í™”
            logger.info(f"[DELETE DEBUG] Disabling batch deletion mode")
            self.batch_deletion_mode = False
            
            # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ ë™ì‹œì— UI ì—…ë°ì´íŠ¸ (100ms ì§€ì—°ìœ¼ë¡œ Google API ì‚­ì œì™€ ë™ê¸°í™”)
            from PyQt6.QtCore import QTimer
            if affected_months:
                def update_all_months():
                    logger.info(f"[DELETE DEBUG] Final UI update after Google deletion completed")
                    for year, month in affected_months:
                        self.data_updated.emit(year, month)
                    logger.info(f"[DELETE DEBUG] UI ì—…ë°ì´íŠ¸ ì™„ë£Œ (ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ë™ì‹œ): {list(affected_months)}")
                
                QTimer.singleShot(100, update_all_months)

    def load_initial_month(self):
        logger.info("ì´ˆê¸° ë°ì´í„° ë¡œë”©ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
        self.get_all_calendars(fetch_if_empty=True)
        today = datetime.date.today()
        self.get_events(today.year, today.month)

    def _apply_colors_to_events(self, events):
        if not events: return
        all_calendars = self.get_all_calendars(fetch_if_empty=False)
        custom_colors = self.settings.get("calendar_colors", {})
        default_color_map = {cal['id']: cal.get('backgroundColor', DEFAULT_EVENT_COLOR) for cal in all_calendars}
        for event in events:
            cal_id = event.get('calendarId')
            if cal_id:
                default_color = default_color_map.get(cal_id, DEFAULT_EVENT_COLOR)
                event['color'] = custom_colors.get(cal_id, default_color)

    def search_events(self, query):
        all_results = []
        for provider in self.providers:
            try:
                results = provider.search_events(query, self)
                if results:
                    all_results.extend(results)
            except Exception as e:
                msg = f"'{type(provider).__name__}'ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                logger.error(msg, exc_info=True)
                self.report_error(f"{msg}\n{e}")
        unique_results = list({event['id']: event for event in all_results}.values())
        self._apply_colors_to_events(unique_results)
        def get_start_time(event):
            start = event.get('start', {})
            return start.get('dateTime') or start.get('date')
        unique_results.sort(key=get_start_time)
        return unique_results

    def _check_for_notifications(self):
        if not self.settings.get("notifications_enabled", DEFAULT_NOTIFICATIONS_ENABLED):
            return

        minutes_before = self.settings.get("notification_minutes", DEFAULT_NOTIFICATION_MINUTES)
        now = datetime.datetime.now().astimezone()
        notification_start_time = now
        notification_end_time = now + datetime.timedelta(minutes=minutes_before)

        today = datetime.date.today()
        events_to_check = []
        
        current_month_events = self.event_cache.get((today.year, today.month), [])
        events_to_check.extend(current_month_events)
        
        next_month_date = today.replace(day=28) + datetime.timedelta(days=4)
        next_month_events = self.event_cache.get((next_month_date.year, next_month_date.month), [])
        events_to_check.extend(next_month_events)

        if self.settings.get("all_day_notification_enabled", DEFAULT_ALL_DAY_NOTIFICATION_ENABLED):
            notification_time_str = self.settings.get("all_day_notification_time", DEFAULT_ALL_DAY_NOTIFICATION_TIME)
            hour, minute = map(int, notification_time_str.split(':'))
            notification_time_today = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

            today_str = today.strftime('%Y-%m-%d')
            all_day_events_today = [
                e for e in current_month_events 
                if e.get('start', {}).get('date') == today_str
            ]

            if now >= notification_time_today:
                for event in all_day_events_today:
                    notification_id = f"{event.get('id')}_{today_str}"
                    if notification_id not in self.notified_event_ids:
                        summary = event.get('summary', 'ì œëª© ì—†ìŒ')
                        message = f"ì˜¤ëŠ˜ '{summary}' ì¼ì •ì´ ìˆìŠµë‹ˆë‹¤."
                        self.notification_triggered.emit("ì¼ì • ì•Œë¦¼", message)
                        self.notified_event_ids.add(notification_id)

        for event in events_to_check:
            event_id = event.get('id')
            if event_id in self.notified_event_ids:
                continue

            start_info = event.get('start', {})
            start_time_str = start_info.get('dateTime')
            
            if not start_time_str:
                continue

            try:
                if start_time_str.endswith('Z'):
                    start_time_str = start_time_str[:-1] + '+00:00'

                event_start_time = datetime.datetime.fromisoformat(start_time_str)
                if event_start_time.tzinfo is None:
                    event_start_time = event_start_time.astimezone()

                if notification_start_time <= event_start_time < notification_end_time:
                    summary = event.get('summary', 'ì œëª© ì—†ìŒ')
                    
                    time_diff = event_start_time - now
                    minutes_remaining = int(time_diff.total_seconds() / 60)
                    
                    if minutes_remaining > 0:
                        message = f"{minutes_remaining}ë¶„ í›„ì— '{summary}' ì¼ì •ì´ ì‹œì‘ë©ë‹ˆë‹¤."
                    else:
                        message = f"ì§€ê¸ˆ '{summary}' ì¼ì •ì´ ì‹œì‘ë©ë‹ˆë‹¤."

                    self.notification_triggered.emit("ì¼ì • ì•Œë¦¼", message)
                    self.notified_event_ids.add(event_id)
                    
                    if len(self.notified_event_ids) > 100:
                         self.notified_event_ids.clear()

            except ValueError:
                continue

    def get_provider_by_name(self, name):
        return next((p for p in self.providers if p.name == name), None)

    def get_classified_events_for_week(self, start_of_week):
        hide_weekends = self.settings.get("hide_weekends", False)
        num_days = 5 if hide_weekends else 7
        
        week_events = self.get_events_for_period(start_of_week, start_of_week + datetime.timedelta(days=num_days-1))
        selected_ids = self.settings.get("selected_calendars", [])
        filtered_events = [event for event in week_events if event.get('calendarId') in selected_ids]

        try:
            user_tz = ZoneInfo(self.settings.get("user_timezone", "UTC"))
        except ZoneInfoNotFoundError:
            user_tz = ZoneInfo("UTC")

        time_events, all_day_events = [], []
        for e in filtered_events:
            try:
                start_str = e['start'].get('dateTime', e['start'].get('date'))
                end_str = e['end'].get('dateTime', e['end'].get('date'))

                if 'dateTime' in e['start']:
                    aware_start_dt = dateutil_parser.isoparse(start_str)
                    aware_end_dt = dateutil_parser.isoparse(end_str)
                    e['start']['local_dt'] = aware_start_dt.astimezone(user_tz)
                    e['end']['local_dt'] = aware_end_dt.astimezone(user_tz)
                else: # ì¢…ì¼ ì´ë²¤íŠ¸
                    naive_start_dt = datetime.datetime.fromisoformat(start_str)
                    naive_end_dt = datetime.datetime.fromisoformat(end_str)
                    e['start']['local_dt'] = naive_start_dt.replace(tzinfo=user_tz) if naive_start_dt.tzinfo is None else naive_start_dt
                    e['end']['local_dt'] = naive_end_dt.replace(tzinfo=user_tz) if naive_end_dt.tzinfo is None else naive_end_dt

                if hide_weekends and e['start']['local_dt'].weekday() >= 5:
                    continue

                is_all_day_native = 'date' in e['start']
                duration = e['end']['local_dt'] - e['start']['local_dt']
                is_multi_day = duration.total_seconds() >= 86400
                is_exactly_24h_midnight = duration.total_seconds() == 86400 and e['start']['local_dt'].time() == datetime.time(0, 0)

                if is_all_day_native or (is_multi_day and not is_exactly_24h_midnight):
                    all_day_events.append(e)
                elif 'dateTime' in e['start']:
                    time_events.append(e)
            except (ValueError, TypeError) as err:
                logger.warning(f"ì£¼ê°„ ë·° ì´ë²¤íŠ¸ ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {err}, ì´ë²¤íŠ¸: {e.get('summary')}")
                continue
        
        return time_events, all_day_events

    def get_events_for_agenda(self, start_date, days=30):
        """
        [ìˆ˜ì •ë¨] ì—¬ëŸ¬ ë‚ ì— ê±¸ì¹œ ì¼ì •ì„ ê° ë‚ ì§œì— ë§ê²Œ í¬í•¨í•˜ë„ë¡ ë¡œì§ì„ ê°œì„ í•©ë‹ˆë‹¤.
        """
        agenda_end_date = start_date + datetime.timedelta(days=days)
        
        # ì‹œì‘ì¼ 30ì¼ ì „ë¶€í„° ì¡°íšŒí•˜ì—¬, í˜„ì¬ ë·°ì— ê±¸ì³ìˆëŠ” ê¸´ ì¼ì •ì„ ë†“ì¹˜ì§€ ì•Šë„ë¡ í•¨
        search_start_date = start_date - datetime.timedelta(days=30)
        events_in_period = self.get_events_for_period(search_start_date, agenda_end_date)

        # ë‚ ì§œë³„ë¡œ ì´ë²¤íŠ¸ë¥¼ ë‹´ì„ OrderedDict ìƒì„±
        agenda = OrderedDict()
        for i in range(days):
            current_day = start_date + datetime.timedelta(days=i)
            agenda[current_day] = []

        for event in events_in_period:
            try:
                start_info = event.get('start', {})
                end_info = event.get('end', {})

                start_str = start_info.get('dateTime', start_info.get('date'))
                end_str = end_info.get('dateTime', end_info.get('date'))

                # Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ fromisoformat í˜¸í™˜ì„± í™•ë³´
                if start_str.endswith('Z'): start_str = start_str[:-1] + '+00:00'
                if end_str.endswith('Z'): end_str = end_str[:-1] + '+00:00'
                
                start_dt = datetime.datetime.fromisoformat(start_str)
                end_dt = datetime.datetime.fromisoformat(end_str)

                # ì¢…ì¼ ì´ë²¤íŠ¸ì˜ ê²½ìš°, end_dateê°€ ë‹¤ìŒ ë‚  0ì‹œë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í•˜ë£¨ë¥¼ ë¹¼ì„œ ì‹¤ì œ ì¢…ë£Œì¼ë¡œ ë§ì¶¤
                if 'date' in start_info:
                    end_dt -= datetime.timedelta(days=1)

                # ì•ˆê±´ ë·°ì˜ ê° ë‚ ì§œë¥¼ ìˆœíšŒí•˜ë©° ì´ë²¤íŠ¸ê°€ í•´ë‹¹ ë‚ ì§œì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                for day in agenda.keys():
                    day_dt_start = datetime.datetime.combine(day, datetime.time.min).astimezone()
                    day_dt_end = datetime.datetime.combine(day, datetime.time.max).astimezone()
                    
                    # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                    if start_dt.tzinfo is None: start_dt = start_dt.astimezone()
                    if end_dt.tzinfo is None: end_dt = end_dt.astimezone()

                    # ì´ë²¤íŠ¸ ê¸°ê°„ì´ í˜„ì¬ ë‚ ì§œì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                    if start_dt <= day_dt_end and end_dt >= day_dt_start:
                        # ìœ„ì ¯ì—ì„œ í˜„ì¬ ë‚ ì§œë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ 'agenda_display_date' ì¶”ê°€
                        event_copy = event.copy()
                        event_copy['agenda_display_date'] = day
                        agenda[day].append(event_copy)

            except (ValueError, TypeError) as e:
                # logger.warning(f"ì•ˆê±´ ë·° ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}, ì´ë²¤íŠ¸: {event.get('summary')}")
                continue
        
        # ê° ë‚ ì§œë³„ë¡œ ì´ë²¤íŠ¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        for day, events in agenda.items():
            events.sort(key=lambda e: e['start'].get('dateTime', e['start'].get('date')))

        # ì´ë²¤íŠ¸ê°€ ì—†ëŠ” ë‚ ì§œëŠ” ì œê±°
        return OrderedDict((day, events) for day, events in agenda.items() if events)

    def get_events_for_period(self, start_date, end_date):
        all_events = []
        months_to_check = set()
        current_date = start_date
        while current_date <= end_date:
            months_to_check.add((current_date.year, current_date.month))
            if current_date.month == 12:
                current_date = current_date.replace(year=current_date.year + 1, month=1, day=1)
            else:
                current_date = current_date.replace(month=current_date.month + 1, day=1)
        for year, month in months_to_check:
            # [ìˆ˜ì •] get_eventsëŠ” ì´ì œ ë¹„ë™ê¸° ìš”ì²­ë§Œ íŠ¸ë¦¬ê±°í•˜ë¯€ë¡œ, ì§ì ‘ ìºì‹œë¥¼ í™•ì¸
            monthly_events = self.event_cache.get((year, month), [])
            for event in monthly_events:
                try:
                    start_info = event.get('start', {})
                    end_info = event.get('end', {})
                    start_str = start_info.get('date') or start_info.get('dateTime', '')[:10]
                    end_str = end_info.get('date') or end_info.get('dateTime', '')[:10]
                    event_start_date = datetime.date.fromisoformat(start_str)
                    event_end_date = datetime.date.fromisoformat(end_str)
                    if 'date' in end_info:
                        event_end_date -= datetime.timedelta(days=1)
                    if not (event_end_date < start_date or event_start_date > end_date):
                        # [ì¶”ê°€] ì‚­ì œ ëŒ€ê¸° ì¤‘ì¸ ì´ë²¤íŠ¸ëŠ” ì œì™¸
                        event_id = event.get('id')
                        if event_id not in self.pending_deletion_ids:
                            all_events.append(event)
                        else:
                            logger.info(f"[DELETE DEBUG] Filtered out pending deletion event: {event_id}")
                except (ValueError, TypeError) as e:
                    print(f"ì´ë²¤íŠ¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}, ì´ë²¤íŠ¸: {event.get('summary')}")
                    continue
        unique_events = {e['id']: e for e in all_events}.values()
        return list(unique_events)

    def get_all_calendars(self, fetch_if_empty=True):
        if self.calendar_list_cache is not None:
            return self.calendar_list_cache

        if fetch_if_empty:
            self._fetch_calendars_async()
        
        return []

    def _fetch_calendars_async(self):
        # [IMPROVED] Direct synchronous execution to avoid threading issues
        logger.info("ìº˜ë¦°ë” ëª©ë¡ ë™ê¸° ë¡œë”© ì‹œì‘...")
        all_calendars = []
        for provider in self.providers:
            if hasattr(provider, 'get_calendars'):
                try:
                    calendars = provider.get_calendars()
                    all_calendars.extend(calendars)
                    logger.info(f"'{type(provider).__name__}'ì—ì„œ {len(calendars)}ê°œ ìº˜ë¦°ë” ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"'{type(provider).__name__}'ì—ì„œ ìº˜ë¦°ë” ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)
        
        self.calendar_list_cache = all_calendars
        logger.info(f"ì´ {len(all_calendars)}ê°œ ìº˜ë¦°ë” ë¡œë”© ì™„ë£Œ")
        
        # UI ì—…ë°ì´íŠ¸ ì‹ í˜¸ ë°œì†¡
        logger.info("calendar_list_changed ì‹ í˜¸ ë°œì†¡ ì‹œì‘...")
        self.calendar_list_changed.emit()
        logger.info("calendar_list_changed ì‹ í˜¸ ë°œì†¡ ì™„ë£Œ")

    def _on_calendar_thread_finished(self):
        if self.calendar_fetch_thread is None: return
            
        self.calendar_fetch_thread.quit()
        if self.calendar_fetch_thread.isRunning():
            self.calendar_fetch_thread.wait()
            
        self.calendar_fetcher.deleteLater()
        self.calendar_fetch_thread.deleteLater()
        self.calendar_fetch_thread = None
        self.calendar_fetcher = None
        logger.info("Calendar list thread cleanup completed")

    def _on_calendars_fetched(self, calendars):
        logger.info(f"Received {len(calendars)} calendars asynchronously")
        self.calendar_list_cache = calendars
        self._default_color_map_cache = None
        self.calendar_list_changed.emit()

    
    def update_event(self, event_data):
        """Local-first event update: UI ì¦‰ì‹œ ì—…ë°ì´íŠ¸ â†’ ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™” (ë°˜ë³µ ì¼ì • í¬í•¨)"""
        # Check for recurrence change first
        recurrence_change_mode = event_data.get('recurrence_change_mode')
        if recurrence_change_mode:
            return self._handle_recurrence_change(event_data)
        
        # Check if calendar has changed (move operation needed)
        original_calendar_id = event_data.get('originalCalendarId')
        original_provider = event_data.get('originalProvider')
        new_calendar_id = event_data.get('calendarId')
        new_provider = event_data.get('provider')
        
        # If calendar changed, perform enhanced move operation for better UX
        if (original_calendar_id and original_provider and 
            (original_calendar_id != new_calendar_id or original_provider != new_provider)):
            
            return self._move_event_between_calendars_atomically(event_data)
        
        # Local-first update with recurring event support
        event_body = event_data.get('body', {})
        event_id = event_body.get('id')
        
        # Check if this is a recurring event
        is_recurring = self._is_recurring_event(event_id)
        
        if is_recurring:
            return self._update_recurring_event_local_first(event_data)
        else:
            return self._update_single_event_local_first(event_data)
        
    def _update_single_event_local_first(self, event_data):
        """ë‹¨ì¼ ì´ë²¤íŠ¸ì˜ Local-first ì—…ë°ì´íŠ¸"""
        event_body = event_data.get('body', {})
        event_id = event_body.get('id')
        
        # 1. ì¦‰ì‹œ ë¡œì»¬ ìºì‹œì—ì„œ ê¸°ì¡´ ì´ë²¤íŠ¸ ì°¾ê¸° ë° ë°±ì—…
        original_event, cache_key = self._find_and_backup_event(event_id)
        if not original_event:
            return False  # ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
        
        # 2. ì¦‰ì‹œ optimistic ì—…ë°ì´íŠ¸ ì ìš©
        updated_event = self._create_optimistic_updated_event(original_event, event_data)
        self._replace_event_in_cache(event_id, updated_event, cache_key)
        
        # 3. ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸
        year, month = cache_key
        self.data_updated.emit(year, month)
        
        # 4. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ë™ê¸°í™”
        self._queue_remote_update_event(event_data, updated_event, original_event)
        
        return True
    
    def _update_recurring_event_local_first(self, event_data):
        """ë°˜ë³µ ì¼ì •ì˜ Local-first ì—…ë°ì´íŠ¸ - ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì¦‰ì‹œ ì—…ë°ì´íŠ¸"""
        try:
            event_body = event_data.get('body', {})
            event_id = event_body.get('id')
            
            # ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ì¶”ì¶œ
            master_event_id = event_id
            if event_id and event_id.startswith('temp_recurring_'):
                # ì„ì‹œ IDì—ì„œ ë§ˆìŠ¤í„° ID ì¶”ì¶œ
                parts = event_id.split('_')
                if len(parts) >= 3:
                    master_event_id = '_'.join(parts[2:-1])  # ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ ì œì™¸
            
            # ëª¨ë“  ìºì‹œì—ì„œ ê´€ë ¨ ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ ì°¾ê¸° ë° ì—…ë°ì´íŠ¸
            affected_months = set()
            instances_updated = 0
            original_instances = []  # ë¡¤ë°±ìš©
            
            for cache_key, events in self.event_cache.items():
                # ì—…ë°ì´íŠ¸í•  ì´ë²¤íŠ¸ë“¤ ì°¾ê¸°
                events_to_update = []
                for i, event in enumerate(events):
                    if (event.get('_master_event_id') == master_event_id or
                        event.get('id') == master_event_id or
                        (event.get('id', '').startswith('temp_recurring_') and 
                         master_event_id in event.get('id', ''))):
                        events_to_update.append((i, event))
                
                # ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸
                if events_to_update:
                    for i, original_event in events_to_update:
                        # ë°±ì—…ë³¸ ì €ì¥ (ë¡¤ë°±ìš©)
                        original_instances.append((cache_key, i, original_event.copy()))
                        
                        # Optimistic ì—…ë°ì´íŠ¸ ì ìš©
                        updated_event = self._create_optimistic_updated_event(original_event, event_data)
                        # ë°˜ë³µ ì´ë²¤íŠ¸ íŠ¹ì„± ìœ ì§€
                        updated_event['_is_recurring_instance'] = original_event.get('_is_recurring_instance', True)
                        updated_event['_instance_index'] = original_event.get('_instance_index', 0)
                        updated_event['_master_event_id'] = original_event.get('_master_event_id', master_event_id)
                        
                        events[i] = updated_event
                        instances_updated += 1
                    
                    affected_months.add(cache_key)
            
            # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ UI ì—…ë°ì´íŠ¸
            for year, month in affected_months:
                self.data_updated.emit(year, month)
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ Google Calendar ì—…ë°ì´íŠ¸
            self._queue_remote_recurring_update(event_data, original_instances)
            
            print(f"[RECURRING UPDATE] Updated {instances_updated} recurring instances in cache")
            return instances_updated > 0
            
        except Exception as e:
            print(f"[RECURRING UPDATE ERROR] Failed to update recurring event: {e}")
            return False
    
    def _find_and_backup_event(self, event_id):
        """ì´ë²¤íŠ¸ IDë¡œ ìºì‹œì—ì„œ ì´ë²¤íŠ¸ ì°¾ê¸° ë° ë°±ì—…"""
        for cache_key, events in self.event_cache.items():
            for event in events:
                if event.get('id') == event_id:
                    # ë°±ì—…ë³¸ ìƒì„± (ë¡¤ë°±ìš©)
                    backup_event = event.copy()
                    return backup_event, cache_key
        return None, None
    
    def _create_optimistic_updated_event(self, original_event, event_data):
        """ê¸°ì¡´ ì´ë²¤íŠ¸ë¥¼ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ìœ¼ë¡œ ì¦‰ì‹œ ìˆ˜ì •"""
        updated_event = original_event.copy()
        
        # ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        event_body = event_data.get('body', {})
        updated_event.update(event_body)
        
        # ë©”íƒ€ ì •ë³´ ì¶”ê°€
        updated_event['calendarId'] = event_data.get('calendarId')
        updated_event['provider'] = event_data.get('provider')
        updated_event['_sync_state'] = 'pending'
        
        # ìƒ‰ìƒ ì •ë³´ ì¬ì„¤ì •
        cal_id = updated_event.get('calendarId')
        if cal_id:
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            cal_info = next((c for c in all_calendars if c['id'] == cal_id), None)
            default_color = cal_info.get('backgroundColor') if cal_info else DEFAULT_EVENT_COLOR
            updated_event['color'] = self.settings.get("calendar_colors", {}).get(cal_id, default_color)
        
        return updated_event
    
    def _replace_event_in_cache(self, event_id, updated_event, cache_key):
        """ìºì‹œì—ì„œ ì´ë²¤íŠ¸ êµì²´"""
        if cache_key in self.event_cache:
            events = self.event_cache[cache_key]
            for i, event in enumerate(events):
                if event.get('id') == event_id:
                    events[i] = updated_event
                    break
    
    def _queue_remote_update_event(self, event_data, updated_event, original_event):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›ê²© ì—…ë°ì´íŠ¸ ë™ê¸°í™”"""
        class RemoteUpdateTask(QRunnable):
            def __init__(self, data_manager, event_data, updated_event, original_event):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.updated_event = updated_event
                self.original_event = original_event
                
            def run(self):
                provider_name = self.event_data.get('provider')
                event_id = self.updated_event['id']
                
                for provider in self.data_manager.providers:
                    if provider.name == provider_name:
                        try:
                            # ì‹¤ì œ ì›ê²© API í˜¸ì¶œ
                            real_updated_event = provider.update_event(self.event_data, self.data_manager)
                            if real_updated_event:
                                # ì„±ê³µ: ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
                                self.data_manager._mark_event_sync_success(event_id, real_updated_event, provider_name)
                            else:
                                # ì‹¤íŒ¨: ì›ë³¸ ì´ë²¤íŠ¸ë¡œ ë¡¤ë°±
                                self.data_manager._rollback_failed_update(event_id, self.original_event, "ì›ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                        except Exception as e:
                            logger.error(f"ì›ê²© ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                            self.data_manager._rollback_failed_update(event_id, self.original_event, str(e))
                        break
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        task = RemoteUpdateTask(self, event_data, updated_event, original_event)
        QThreadPool.globalInstance().start(task)
    
    def _handle_recurrence_change(self, event_data):
        """ë°˜ë³µ ê·œì¹™ ë³€ê²½ ì²˜ë¦¬"""
        recurrence_change_mode = event_data.get('recurrence_change_mode')
        recurrence_change_option = event_data.get('recurrence_change_option', 'all')
        event_body = event_data.get('body', {})
        event_id = event_body.get('id')
        
        logger.info(f"ë°˜ë³µ ê·œì¹™ ë³€ê²½ ì²˜ë¦¬: mode={recurrence_change_mode}, option={recurrence_change_option}, id={event_id}")
        
        if recurrence_change_mode == "single_to_recurring":
            return self._convert_single_to_recurring(event_data)
        elif recurrence_change_mode == "recurring_to_single":
            return self._convert_recurring_to_single(event_data, recurrence_change_option)
        elif recurrence_change_mode == "modify_recurrence":
            return self._modify_recurrence_rule(event_data, recurrence_change_option)
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°˜ë³µ ê·œì¹™ ë³€ê²½ ëª¨ë“œ: {recurrence_change_mode}")
            return False
    
    def _convert_single_to_recurring(self, event_data):
        """ë‹¨ì¼ ì¼ì •ì„ ë°˜ë³µ ì¼ì •ìœ¼ë¡œ ë³€í™˜"""
        try:
            event_body = event_data.get('body', {})
            event_id = event_body.get('id')
            
            # 1. ê¸°ì¡´ ë‹¨ì¼ ì¼ì • ì°¾ê¸° ë° ì‚­ì œ
            original_event, cache_key = self._find_and_backup_event(event_id)
            if not original_event:
                logger.error(f"ë³€í™˜í•  ë‹¨ì¼ ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {event_id}")
                return False
            
            # ê¸°ì¡´ ì¼ì •ì„ ìºì‹œì—ì„œ ì œê±°
            if cache_key and cache_key in self.event_cache:
                self.event_cache[cache_key] = [e for e in self.event_cache[cache_key] if e.get('id') != event_id]
            
            # 2. ìƒˆë¡œìš´ ë°˜ë³µ ì¼ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            updated_event = self._create_optimistic_updated_event(original_event, event_data)
            
            # 3. ë°˜ë³µ ì¼ì • ì¸ìŠ¤í„´ìŠ¤ë“¤ ìƒì„± ë° ìºì‹œì— ì¶”ê°€
            self._generate_recurring_instances_for_cache(updated_event)
            
            # 4. UI ì—…ë°ì´íŠ¸
            if cache_key:
                year, month = cache_key
                self.data_updated.emit(year, month)
            
            # 5. ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™”
            self._queue_remote_recurrence_conversion(event_data, original_event, "single_to_recurring")
            
            logger.info(f"ë‹¨ì¼ â†’ ë°˜ë³µ ì¼ì • ë³€í™˜ ì™„ë£Œ: {event_id}")
            return True
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ â†’ ë°˜ë³µ ì¼ì • ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    def _convert_recurring_to_single(self, event_data, option):
        """ë°˜ë³µ ì¼ì •ì„ ë‹¨ì¼ ì¼ì •ìœ¼ë¡œ ë³€í™˜"""
        try:
            event_body = event_data.get('body', {})
            event_id = event_body.get('id')
            
            if option == "instance":
                # ì´ ì¸ìŠ¤í„´ìŠ¤ë§Œ ë‹¨ì¼ ì¼ì •ìœ¼ë¡œ ë³€í™˜
                return self._convert_single_instance_to_single(event_data)
            elif option == "future":
                # ì´í›„ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‹¨ì¼ ì¼ì •ìœ¼ë¡œ ë³€í™˜ (ë°˜ë³µ ì¢…ë£Œ)
                return self._convert_future_instances_to_single(event_data)
            elif option == "all":
                # ëª¨ë“  ë°˜ë³µì„ ì‚­ì œí•˜ê³  í•˜ë‚˜ì˜ ë‹¨ì¼ ì¼ì •ìœ¼ë¡œ ë³€í™˜
                return self._convert_all_recurring_to_single(event_data)
            else:
                logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë³€í™˜ ì˜µì…˜: {option}")
                return False
                
        except Exception as e:
            logger.error(f"ë°˜ë³µ â†’ ë‹¨ì¼ ì¼ì • ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    def _modify_recurrence_rule(self, event_data, option):
        """ë°˜ë³µ ê·œì¹™ ìˆ˜ì •"""
        try:
            event_body = event_data.get('body', {})
            event_id = event_body.get('id')
            original_rrule = event_data.get('original_rrule')
            new_rrule = event_body.get('recurrence', [None])[0] if event_body.get('recurrence') else None
            
            if option == "future":
                # ì´í›„ ëª¨ë“  ì¼ì •ì˜ ë°˜ë³µ ê·œì¹™ ë³€ê²½
                return self._modify_future_recurrence(event_data, original_rrule, new_rrule)
            elif option == "all":
                # ëª¨ë“  ë°˜ë³µ ì¼ì •ì˜ ê·œì¹™ ë³€ê²½
                return self._modify_all_recurrence(event_data, original_rrule, new_rrule)
            else:
                logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ìˆ˜ì • ì˜µì…˜: {option}")
                return False
                
        except Exception as e:
            logger.error(f"ë°˜ë³µ ê·œì¹™ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _convert_single_instance_to_single(self, event_data):
        """ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë§Œ ì¼ë°˜ ì¼ì •ìœ¼ë¡œ ë³€í™˜ (ë°˜ë³µì—ì„œ ì œì™¸)"""
        # ì´ëŠ” ì˜ˆì™¸ ì²˜ë¦¬ì™€ ë™ì¼í•œ ë¡œì§ (ë°˜ë³µì—ì„œ í•´ë‹¹ ë‚ ì§œ ì œì™¸)
        event_body = event_data.get('body', {})
        event_id = event_body.get('id')
        
        # ì˜ˆì™¸ ë‚ ì§œë¡œ ì¶”ê°€
        original_id = event_id.split('_')[0] if '_' in event_id else event_id
        start_str = event_body.get('start', {}).get('dateTime') or event_body.get('start', {}).get('date')
        
        if start_str:
            try:
                import datetime
                if 'T' in start_str:
                    exception_date = datetime.datetime.fromisoformat(start_str.replace('Z', '+00:00')).isoformat()
                else:
                    exception_date = start_str
                    
                # ë¡œì»¬ ì œê³µìì— ì˜ˆì™¸ ì¶”ê°€
                for provider in self.providers:
                    if provider.name == LOCAL_CALENDAR_PROVIDER_NAME:
                        with provider._get_connection() as conn:
                            cursor = conn.cursor()
                            cursor.execute(
                                "INSERT OR IGNORE INTO event_exceptions (original_event_id, exception_date) VALUES (?, ?)",
                                (original_id, exception_date)
                            )
                            conn.commit()
                        break
                
                # ìƒˆë¡œìš´ ë‹¨ì¼ ì¼ì • ìƒì„±
                event_body['id'] = str(uuid.uuid4())  # ìƒˆë¡œìš´ ID
                event_body.pop('recurrence', None)  # ë°˜ë³µ ì œê±°
                
                # ìºì‹œì— ë‹¨ì¼ ì¼ì •ìœ¼ë¡œ ì¶”ê°€
                updated_event = self._create_optimistic_updated_event({}, event_data)
                self._add_event_to_cache(updated_event)
                
                return True
                
            except Exception as e:
                logger.error(f"ì¸ìŠ¤í„´ìŠ¤ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return False
        
        return False
    
    def _generate_recurring_instances_for_cache(self, master_event):
        """ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ë¡œë¶€í„° ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ìƒì„±í•˜ì—¬ ìºì‹œì— ì¶”ê°€"""
        # ì´ëŠ” ê¸°ì¡´ ìºì‹± ë¡œì§ì„ í™œìš©í•˜ì—¬ ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ë“¤ ìƒì„±
        try:
            import datetime
            from dateutil.rrule import rrulestr
            
            recurrence = master_event.get('recurrence', [])
            if not recurrence:
                return
            
            rrule_str = recurrence[0]
            start_str = master_event.get('start', {}).get('dateTime') or master_event.get('start', {}).get('date')
            
            if not start_str:
                return
            
            # í˜„ì¬ ë³´ì´ëŠ” ë‹¬ë“¤ì˜ ë²”ìœ„ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            current_date = datetime.date.today()
            for month_offset in range(-3, 4):  # í˜„ì¬ ê¸°ì¤€ Â±3ê°œì›”
                target_date = current_date.replace(day=1)
                if month_offset != 0:
                    year = target_date.year
                    month = target_date.month + month_offset
                    while month <= 0:
                        month += 12
                        year -= 1
                    while month > 12:
                        month -= 12
                        year += 1
                    target_date = target_date.replace(year=year, month=month)
                
                # í•´ë‹¹ ì›”ì˜ ì¸ìŠ¤í„´ìŠ¤ë“¤ ìƒì„±
                month_start = target_date
                if target_date.month == 12:
                    month_end = target_date.replace(year=target_date.year+1, month=1, day=1) - datetime.timedelta(days=1)
                else:
                    month_end = target_date.replace(month=target_date.month+1, day=1) - datetime.timedelta(days=1)
                
                # RRULE íŒŒì‹±í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (local_provider ë¡œì§ í™œìš©)
                for provider in self.providers:
                    if provider.name == LOCAL_CALENDAR_PROVIDER_NAME:
                        instances = provider.get_events(month_start, month_end, self)
                        recurring_instances = [e for e in instances if e.get('id', '').startswith(master_event['id'])]
                        
                        cache_key = (target_date.year, target_date.month)
                        if cache_key not in self.event_cache:
                            self.event_cache[cache_key] = []
                        
                        # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ ì œê±° í›„ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
                        self.event_cache[cache_key] = [e for e in self.event_cache[cache_key] 
                                                     if not e.get('id', '').startswith(master_event['id'])]
                        self.event_cache[cache_key].extend(recurring_instances)
                        break
                        
        except Exception as e:
            logger.error(f"ë°˜ë³µ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _queue_remote_recurrence_conversion(self, event_data, original_event, conversion_type):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°˜ë³µ ê·œì¹™ ë³€í™˜ ë™ê¸°í™”"""
        class RecurrenceConversionTask(QRunnable):
            def __init__(self, data_manager, event_data, original_event, conversion_type):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.original_event = original_event
                self.conversion_type = conversion_type
                
            def run(self):
                provider_name = self.event_data.get('provider')
                
                for provider in self.data_manager.providers:
                    if provider.name == provider_name:
                        try:
                            if self.conversion_type == "single_to_recurring":
                                # ê¸°ì¡´ ë‹¨ì¼ ì¼ì • ì‚­ì œ í›„ ìƒˆë¡œìš´ ë°˜ë³µ ì¼ì • ì¶”ê°€
                                provider.delete_event({'body': self.original_event}, self.data_manager)
                                result = provider.add_event(self.event_data, self.data_manager)
                            else:
                                # ê¸°íƒ€ ë³€í™˜ íƒ€ì…ë“¤
                                result = provider.update_event(self.event_data, self.data_manager)
                            
                            if result:
                                logger.info(f"ë°˜ë³µ ê·œì¹™ ë³€í™˜ ì›ê²© ë™ê¸°í™” ì„±ê³µ: {self.conversion_type}")
                            else:
                                logger.error(f"ë°˜ë³µ ê·œì¹™ ë³€í™˜ ì›ê²© ë™ê¸°í™” ì‹¤íŒ¨: {self.conversion_type}")
                                
                        except Exception as e:
                            logger.error(f"ë°˜ë³µ ê·œì¹™ ë³€í™˜ ì›ê²© ë™ê¸°í™” ì˜¤ë¥˜: {e}")
                        break
        
        task = RecurrenceConversionTask(self, event_data, original_event, conversion_type)
        QThreadPool.globalInstance().start(task)
    
    def _queue_remote_recurring_update(self, event_data, original_instances):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°˜ë³µ ì¼ì • ì›ê²© ì—…ë°ì´íŠ¸"""
        class RemoteRecurringUpdateTask(QRunnable):
            def __init__(self, data_manager, event_data, original_instances):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.original_instances = original_instances
            
            def run(self):
                try:
                    # ì‹¤ì œ Google Calendar ì—…ë°ì´íŠ¸ ì²˜ë¦¬
                    provider_name = self.event_data.get('provider')
                    for provider in self.data_manager.providers:
                        if provider.name == provider_name:
                            # ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸ (Googleì´ ë°˜ë³µ í™•ì¥ ì²˜ë¦¬)
                            real_updated_event = provider.update_event(self.event_data, self.data_manager)
                            if real_updated_event:
                                # ì„±ê³µ: ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì˜ ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
                                self.data_manager._mark_recurring_sync_success(self.event_data, real_updated_event)
                            else:
                                # ì‹¤íŒ¨: ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì›ë³¸ìœ¼ë¡œ ë¡¤ë°±
                                self.data_manager._rollback_failed_recurring_update(self.original_instances, "ì›ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                            break
                except Exception as e:
                    print(f"[REMOTE RECURRING UPDATE ERROR] {e}")
                    self.data_manager._rollback_failed_recurring_update(self.original_instances, str(e))
        
        task = RemoteRecurringUpdateTask(self, event_data, original_instances)
        QThreadPool.globalInstance().start(task)
    
    def _mark_recurring_sync_success(self, event_data, real_event):
        """ë°˜ë³µ ì¼ì • ë™ê¸°í™” ì„±ê³µ ì²˜ë¦¬"""
        event_body = event_data.get('body', {})
        master_event_id = event_body.get('id')
        
        # ë§ˆìŠ¤í„° ì´ë²¤íŠ¸ ID ì¶”ì¶œ
        if master_event_id and master_event_id.startswith('temp_recurring_'):
            parts = master_event_id.split('_')
            if len(parts) >= 3:
                master_event_id = '_'.join(parts[2:-1])
        
        # ëª¨ë“  ê´€ë ¨ ì¸ìŠ¤í„´ìŠ¤ì˜ ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
        affected_months = set()
        for cache_key, events in self.event_cache.items():
            for event in events:
                if (event.get('_master_event_id') == master_event_id or
                    event.get('id') == master_event_id or
                    (event.get('id', '').startswith('temp_recurring_') and 
                     master_event_id in event.get('id', ''))):
                    event['_sync_state'] = 'synced'
                    affected_months.add(cache_key)
        
        # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ UI ì—…ë°ì´íŠ¸
        for year, month in affected_months:
            self.data_updated.emit(year, month)
        
        print(f"[RECURRING SYNC SUCCESS] Updated sync status for recurring event {master_event_id}")
    
    def _rollback_failed_recurring_update(self, original_instances, error_msg):
        """ë°˜ë³µ ì¼ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±"""
        affected_months = set()
        
        for cache_key, index, original_event in original_instances:
            if cache_key in self.event_cache:
                # ì‹¤íŒ¨ ìƒíƒœ í‘œì‹œ
                original_event['_sync_state'] = 'failed'
                original_event['_sync_error'] = error_msg
                
                # ì›ë³¸ ì´ë²¤íŠ¸ë¡œ ë³µì›
                self.event_cache[cache_key][index] = original_event
                affected_months.add(cache_key)
        
        # ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ UI ì—…ë°ì´íŠ¸ (ë¡¤ë°± ìƒíƒœ í‘œì‹œ)
        for year, month in affected_months:
            self.data_updated.emit(year, month)
        
        print(f"[RECURRING UPDATE ROLLBACK] Rolled back {len(original_instances)} instances due to: {error_msg}")
    
    def _mark_event_sync_success(self, event_id, real_event, provider_name):
        """ì´ë²¤íŠ¸ ë™ê¸°í™” ì„±ê³µ ì²˜ë¦¬"""
        if 'provider' not in real_event:
            real_event['provider'] = provider_name
        
        # ìƒ‰ìƒ ì •ë³´ ì¶”ê°€
        cal_id = real_event.get('calendarId')
        if cal_id:
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            cal_info = next((c for c in all_calendars if c['id'] == cal_id), None)
            default_color = cal_info.get('backgroundColor') if cal_info else DEFAULT_EVENT_COLOR
            real_event['color'] = self.settings.get("calendar_colors", {}).get(cal_id, default_color)
        
        real_event['_sync_state'] = 'synced'
        
        # ìºì‹œì—ì„œ ì´ë²¤íŠ¸ êµì²´
        for cache_key, events in self.event_cache.items():
            for i, event in enumerate(events):
                if event.get('id') == event_id:
                    events[i] = real_event
                    # UI ì—…ë°ì´íŠ¸
                    year, month = cache_key
                    self.data_updated.emit(year, month)
                    return
    
    def _rollback_failed_update(self, event_id, original_event, error_msg):
        """ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±"""
        original_event['_sync_state'] = 'failed'
        original_event['_sync_error'] = error_msg
        
        # ìºì‹œì—ì„œ ì›ë³¸ ì´ë²¤íŠ¸ë¡œ ë³µì›
        for cache_key, events in self.event_cache.items():
            for i, event in enumerate(events):
                if event.get('id') == event_id:
                    events[i] = original_event
                    # UI ì—…ë°ì´íŠ¸í•˜ì—¬ ë¡¤ë°± ìƒíƒœ í‘œì‹œ
                    year, month = cache_key
                    self.data_updated.emit(year, month)
                    return
    

    def load_initial_month(self):
        logger.info("Requesting initial data loading...")
        self.get_all_calendars(fetch_if_empty=True)
        today = datetime.date.today()
        self.get_events(today.year, today.month)

    def _apply_colors_to_events(self, events):
        if not events: return
        all_calendars = self.get_all_calendars(fetch_if_empty=False)
        custom_colors = self.settings.get("calendar_colors", {})
        default_color_map = {cal['id']: cal.get('backgroundColor', DEFAULT_EVENT_COLOR) for cal in all_calendars}
        for event in events:
            cal_id = event.get('calendarId')
            if cal_id:
                default_color = default_color_map.get(cal_id, DEFAULT_EVENT_COLOR)
                event['color'] = custom_colors.get(cal_id, default_color)

    def search_events(self, query):
        all_results = []
        for provider in self.providers:
            try:
                results = provider.search_events(query, self)
                if results:
                    all_results.extend(results)
            except Exception as e:
                print(f"'{type(provider).__name__}' ì´ë²¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        unique_results = list({event['id']: event for event in all_results}.values())
        self._apply_colors_to_events(unique_results)
        def get_start_time(event):
            start = event.get('start', {})
            return start.get('dateTime') or start.get('date')
        unique_results.sort(key=get_start_time)
        return unique_results

    def _check_for_notifications(self):
        if not self.settings.get("notifications_enabled", DEFAULT_NOTIFICATIONS_ENABLED):
            return

        minutes_before = self.settings.get("notification_minutes", DEFAULT_NOTIFICATION_MINUTES)
        now = datetime.datetime.now().astimezone()
        notification_start_time = now
        notification_end_time = now + datetime.timedelta(minutes=minutes_before)

        today = datetime.date.today()
        events_to_check = []
        
        current_month_events = self.event_cache.get((today.year, today.month), [])
        events_to_check.extend(current_month_events)
        
        next_month_date = today.replace(day=28) + datetime.timedelta(days=4)
        next_month_events = self.event_cache.get((next_month_date.year, next_month_date.month), [])
        events_to_check.extend(next_month_events)

        if self.settings.get("all_day_notification_enabled", DEFAULT_ALL_DAY_NOTIFICATION_ENABLED):
            notification_time_str = self.settings.get("all_day_notification_time", DEFAULT_ALL_DAY_NOTIFICATION_TIME)
            hour, minute = map(int, notification_time_str.split(':'))
            notification_time_today = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

            today_str = today.strftime('%Y-%m-%d')
            all_day_events_today = [
                e for e in current_month_events 
                if e.get('start', {}).get('date') == today_str
            ]

            if now >= notification_time_today:
                for event in all_day_events_today:
                    notification_id = f"{event.get('id')}_{today_str}"
                    if notification_id not in self.notified_event_ids:
                        summary = event.get('summary', 'ì œëª© ì—†ìŒ')
                        message = f"ì˜¤ëŠ˜ '{summary}' ì¼ì •ì´ ìˆìŠµë‹ˆë‹¤."
                        self.notification_triggered.emit("ì¼ì • ì•Œë¦¼", message)
                        self.notified_event_ids.add(notification_id)

        for event in events_to_check:
            event_id = event.get('id')
            if event_id in self.notified_event_ids:
                continue

            start_info = event.get('start', {})
            start_time_str = start_info.get('dateTime')
            
            if not start_time_str:
                continue

            try:
                if start_time_str.endswith('Z'):
                    start_time_str = start_time_str[:-1] + '+00:00'

                event_start_time = datetime.datetime.fromisoformat(start_time_str)
                if event_start_time.tzinfo is None:
                    event_start_time = event_start_time.astimezone()

                if notification_start_time <= event_start_time < notification_end_time:
                    summary = event.get('summary', 'ì œëª© ì—†ìŒ')
                    
                    time_diff = event_start_time - now
                    minutes_remaining = int(time_diff.total_seconds() / 60)
                    
                    if minutes_remaining > 0:
                        message = f"{minutes_remaining}ë¶„ í›„ì— '{summary}' ì¼ì •ì´ ì‹œì‘ë©ë‹ˆë‹¤."
                    else:
                        message = f"ì§€ê¸ˆ '{summary}' ì¼ì •ì´ ì‹œì‘ë©ë‹ˆë‹¤."

                    self.notification_triggered.emit("ì¼ì • ì•Œë¦¼", message)
                    self.notified_event_ids.add(event_id)
                    
                    if len(self.notified_event_ids) > 100:
                         self.notified_event_ids.clear()

            except ValueError:
                continue
    
    # Enhanced Calendar Move Methods
    def _move_event_between_calendars_atomically(self, event_data):
        """
        Enhanced Local-First ìº˜ë¦°ë” ê°„ ì´ë²¤íŠ¸ ì´ë™
        UI ì¦‰ì‹œ ë°˜ì˜ (ì›ìì ) â†’ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ â†’ ì‹¤íŒ¨ì‹œ ë¡¤ë°±
        """
        original_calendar_id = event_data.get('originalCalendarId')
        original_provider = event_data.get('originalProvider')
        new_calendar_id = event_data.get('calendarId')
        new_provider = event_data.get('provider')
        event_id = event_data.get('body', {}).get('id')
        
        print(f"DEBUG: Atomically moving event {event_id} from {original_provider}:{original_calendar_id} to {new_provider}:{new_calendar_id}")
        
        # 1. ì¦‰ì‹œ ìºì‹œì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì›ìì ìœ¼ë¡œ ì´ë™ (ì¤‘ê°„ ìƒíƒœ ì—†ìŒ)
        success = self._move_event_in_cache_atomically(
            event_id, 
            original_calendar_id, 
            original_provider,
            new_calendar_id, 
            new_provider,
            event_data.get('body', {})
        )
        
        if not success:
            print(f"DEBUG: Failed to find event {event_id} in cache for atomic move")
            return False
        
        # 2. ì¦‰ì‹œ UI ì—…ë°ì´íŠ¸
        self._emit_data_updated_for_affected_months(event_data)
        
        # 3. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ ìº˜ë¦°ë” ê°„ ì´ë™ ì²˜ë¦¬
        self._queue_remote_calendar_move(event_data, original_calendar_id, original_provider)
        
        return True

    def _move_event_in_cache_atomically(self, event_id, from_cal_id, from_provider, to_cal_id, to_provider, event_body):
        """
        ìºì‹œì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì›ìì ìœ¼ë¡œ ì´ë™ (ì‚¬ìš©ìê°€ ì¤‘ê°„ ê³¼ì •ì„ ë³´ì§€ ì•ŠìŒ)
        """
        # ì›ë³¸ ì´ë²¤íŠ¸ ì°¾ê¸°
        original_event = None
        source_cache_key = None
        
        for cache_key, events in self.event_cache.items():
            for event in events:
                if (event.get('id') == event_id and 
                    event.get('calendarId') == from_cal_id and
                    event.get('provider') == from_provider):
                    original_event = event.copy()
                    source_cache_key = cache_key
                    break
            if original_event:
                break
        
        if not original_event:
            print(f"DEBUG: Could not find event {event_id} in cache for atomic move")
            return False
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±
        moved_event = original_event.copy()
        moved_event.update(event_body)
        moved_event['calendarId'] = to_cal_id
        moved_event['provider'] = to_provider
        moved_event['_move_state'] = 'moving'  # ì´ë™ ì¤‘ ìƒíƒœ í‘œì‹œ
        moved_event['_original_location'] = {
            'calendarId': from_cal_id,
            'provider': from_provider
        }
        
        # [NEW] ì¦‰ì‹œ íƒ€ê²Ÿ ìº˜ë¦°ë” ìƒ‰ìƒìœ¼ë¡œ ë³€ê²½ (ì‹œê°ì  ì¦‰ì‹œ ë³€í™”)
        target_color = self._get_calendar_color(to_cal_id)
        if target_color:
            moved_event['color'] = target_color
            print(f"DEBUG: Event color changed immediately to {target_color} for calendar {to_cal_id}")
        
        # ì›ìì  ì´ë™: ê¸°ì¡´ ì œê±° + ìƒˆ ìœ„ì¹˜ ì¶”ê°€ (í•œ ë²ˆì— ì²˜ë¦¬)
        try:
            # 1. ì›ë³¸ ì œê±°
            if source_cache_key in self.event_cache:
                self.event_cache[source_cache_key] = [
                    e for e in self.event_cache[source_cache_key] 
                    if not (e.get('id') == event_id and 
                           e.get('calendarId') == from_cal_id and
                           e.get('provider') == from_provider)
                ]
            
            # 2. ìƒˆ ìœ„ì¹˜ì— ì¶”ê°€
            target_cache_key = self._determine_cache_key_for_event(moved_event)
            if target_cache_key not in self.event_cache:
                self.event_cache[target_cache_key] = []
            self.event_cache[target_cache_key].append(moved_event)
            
            print(f"DEBUG: Event {event_id} moved atomically in cache from {from_cal_id} to {to_cal_id}")
            return True
            
        except Exception as e:
            print(f"DEBUG: Failed to move event atomically: {e}")
            return False
    
    def _determine_cache_key_for_event(self, event):
        """ì´ë²¤íŠ¸ì— ì í•©í•œ ìºì‹œ í‚¤ ê²°ì •"""
        start_info = event.get('start', {})
        start_date_str = start_info.get('dateTime') or start_info.get('date', '')
        
        if start_date_str:
            try:
                if 'T' in start_date_str:
                    date_part = start_date_str.split('T')[0]
                else:
                    date_part = start_date_str[:10]
                    
                event_date = datetime.datetime.fromisoformat(date_part).date()
                return (event_date.year, event_date.month)
            except:
                pass
        
        # ê¸°ë³¸ê°’: í˜„ì¬ ì›”
        today = datetime.date.today()
        return (today.year, today.month)
    
    def _get_calendar_color(self, calendar_id):
        """ìº˜ë¦°ë” IDì— í•´ë‹¹í•˜ëŠ” ìƒ‰ìƒ ë°˜í™˜"""
        try:
            # 1. ì‚¬ìš©ì ì„¤ì • ìƒ‰ìƒ í™•ì¸
            custom_colors = self.settings.get("calendar_colors", {})
            if calendar_id in custom_colors:
                return custom_colors[calendar_id]
            
            # 2. ìº˜ë¦°ë” ê¸°ë³¸ ìƒ‰ìƒ í™•ì¸
            all_calendars = self.get_all_calendars(fetch_if_empty=False)
            cal_info = next((c for c in all_calendars if c['id'] == calendar_id), None)
            if cal_info and cal_info.get('backgroundColor'):
                return cal_info['backgroundColor']
            
            # 3. ê¸°ë³¸ ìƒ‰ìƒ ë°˜í™˜
            return DEFAULT_EVENT_COLOR
            
        except Exception as e:
            print(f"DEBUG: Error getting calendar color for {calendar_id}: {e}")
            return DEFAULT_EVENT_COLOR
    
    def _emit_data_updated_for_affected_months(self, event_data):
        """ì˜í–¥ë°›ëŠ” ëª¨ë“  ì›”ì— ëŒ€í•´ UI ì—…ë°ì´íŠ¸ ì‹ í˜¸ ë°œì†¡"""
        # ì›ë³¸ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
        try:
            original_event = {'start': event_data.get('body', {}).get('start', {})}
            original_cache_key = self._determine_cache_key_for_event(original_event)
            year, month = original_cache_key
            self.data_updated.emit(year, month)
        except:
            pass
        
        # ìƒˆ ìœ„ì¹˜ ì—…ë°ì´íŠ¸  
        try:
            new_cache_key = self._determine_cache_key_for_event(event_data.get('body', {}))
            year, month = new_cache_key
            self.data_updated.emit(year, month)
        except:
            pass
    
    def _queue_remote_calendar_move(self, event_data, original_calendar_id, original_provider):
        """
        ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ ìº˜ë¦°ë” ê°„ ì´ë™ ì²˜ë¦¬
        """
        class RemoteMoveTask(QRunnable):
            def __init__(self, data_manager, event_data, original_calendar_id, original_provider):
                super().__init__()
                self.data_manager = data_manager
                self.event_data = event_data
                self.original_calendar_id = original_calendar_id
                self.original_provider = original_provider
                
            def run(self):
                event_id = self.event_data.get('body', {}).get('id')
                
                try:
                    print(f"DEBUG: Starting remote calendar move for event {event_id}")
                    
                    # [FIX] 1. ë¨¼ì € ì›ë³¸ ìº˜ë¦°ë”ì—ì„œ ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
                    original_event_data = {
                        'calendarId': self.original_calendar_id,
                        'provider': self.original_provider,
                        'body': self.event_data.get('body', {})
                    }
                    
                    delete_success = self.data_manager._delete_event_remote_only(original_event_data)
                    
                    if not delete_success:
                        print(f"WARNING: Failed to delete original event {event_id}")
                        # ì‚­ì œ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰ (ì´ë¯¸ ìºì‹œì—ì„œëŠ” ì´ë™ë¨)
                    
                    # 2. ìƒˆ ìº˜ë¦°ë”ì— ì¶”ê°€
                    new_event_data = self.event_data.copy()
                    new_event_data.pop('originalCalendarId', None)
                    new_event_data.pop('originalProvider', None)
                    
                    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ ì¶”ê°€ (ìºì‹œ ì—…ë°ì´íŠ¸ ì—†ì´)
                    add_success = self.data_manager._add_event_remote_only(new_event_data)
                    
                    if not add_success:
                        print(f"ERROR: Failed to add event to new calendar after deletion")
                        # ì´ ê²½ìš°ëŠ” ë¡¤ë°±ì´ í•„ìš”í•¨
                        raise Exception("Failed to add event to new calendar")
                    
                    # 3. ì„±ê³µ: ì´ë™ ìƒíƒœ í´ë¦¬ì–´
                    self.data_manager._clear_move_state(event_id)
                    print(f"DEBUG: Successfully moved event {event_id} between calendars")
                    
                except Exception as e:
                    print(f"DEBUG: Remote calendar move failed for event {event_id}: {e}")
                    # 4. ì‹¤íŒ¨: ë¡¤ë°±
                    self.data_manager._rollback_calendar_move(
                        event_id, 
                        self.original_calendar_id, 
                        self.original_provider,
                        self.event_data.get('calendarId'),
                        self.event_data.get('provider')
                    )
                    # ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ
                    self.data_manager.error_occurred.emit(f"ìº˜ë¦°ë” ì´ë™ ì‹¤íŒ¨: {str(e)}")
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
        task = RemoteMoveTask(self, event_data, original_calendar_id, original_provider)
        QThreadPool.globalInstance().start(task)
    
    def _add_event_remote_only(self, event_data):
        """ì›ê²©ì—ë§Œ ì´ë²¤íŠ¸ ì¶”ê°€ (ìºì‹œ ì—…ë°ì´íŠ¸ ì—†ì´)"""
        try:
            provider_name = event_data.get('provider')
            for provider in self.providers:
                if provider.name == provider_name:
                    # [FIX] data_managerë¥¼ Noneìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ìºì‹œ ì—…ë°ì´íŠ¸ ë°©ì§€
                    result = provider.add_event(event_data, None)
                    return result is not None
            return False
        except Exception as e:
            print(f"DEBUG: _add_event_remote_only failed: {e}")
            return False
    
    def _delete_event_remote_only(self, event_data):
        """ì›ê²©ì—ë§Œ ì´ë²¤íŠ¸ ì‚­ì œ (ìºì‹œ ì—…ë°ì´íŠ¸ ì—†ì´)"""
        try:
            provider_name = event_data.get('provider')
            for provider in self.providers:
                if provider.name == provider_name:
                    # [FIX] data_managerë¥¼ Noneìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ìºì‹œ ì—…ë°ì´íŠ¸ ë°©ì§€
                    result = provider.delete_event(event_data, None)
                    return result
            return False
        except Exception as e:
            print(f"DEBUG: _delete_event_remote_only failed: {e}")
            return False
    
    def _clear_move_state(self, event_id):
        """ì´ë²¤íŠ¸ì˜ ì´ë™ ìƒíƒœ í´ë¦¬ì–´"""
        for cache_key, events in self.event_cache.items():
            for event in events:
                if event.get('id') == event_id:
                    event.pop('_move_state', None)
                    event.pop('_original_location', None)
                    # UI ì—…ë°ì´íŠ¸
                    year, month = cache_key
                    self.data_updated.emit(year, month)
                    break
    
    def _rollback_calendar_move(self, event_id, original_cal_id, original_provider, failed_cal_id, failed_provider):
        """ìº˜ë¦°ë” ì´ë™ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±"""
        print(f"DEBUG: Rolling back calendar move for event {event_id}")
        
        # ì‹¤íŒ¨í•œ ìœ„ì¹˜ì—ì„œ ì´ë²¤íŠ¸ ì°¾ê¸°
        for cache_key, events in self.event_cache.items():
            for i, event in enumerate(events):
                if (event.get('id') == event_id and 
                    event.get('calendarId') == failed_cal_id):
                    
                    # ì›ë³¸ ìœ„ì¹˜ ì •ë³´ ë³µì›
                    original_location = event.get('_original_location', {})
                    rolled_back_event = event.copy()
                    rolled_back_event['calendarId'] = original_location.get('calendarId', original_cal_id)
                    rolled_back_event['provider'] = original_location.get('provider', original_provider)
                    rolled_back_event['_move_state'] = 'failed'
                    rolled_back_event.pop('_original_location', None)
                    
                    # ì‹¤íŒ¨í•œ ìœ„ì¹˜ì—ì„œ ì œê±°
                    events.pop(i)
                    
                    # ì›ë³¸ ìœ„ì¹˜ë¡œ ë³µì›
                    original_cache_key = self._determine_cache_key_for_event(rolled_back_event)
                    if original_cache_key not in self.event_cache:
                        self.event_cache[original_cache_key] = []
                    self.event_cache[original_cache_key].append(rolled_back_event)
                    
                    # UI ì—…ë°ì´íŠ¸
                    self.data_updated.emit(original_cache_key[0], original_cache_key[1])
                    if cache_key != original_cache_key:
                        self.data_updated.emit(cache_key[0], cache_key[1])
                    
                    print(f"DEBUG: Event {event_id} rolled back to original calendar {original_cal_id}")
                    return
        
        print(f"DEBUG: Could not find event {event_id} for rollback")